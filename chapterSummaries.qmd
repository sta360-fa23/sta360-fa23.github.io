---
title: "Chapter summaries"
mainfont: Lato
format: 
  html:
    toc: true
---

### Chapter 2

*Axioms of probability*

For all sets $F$, $G$ and $H$,

- $0 = Pr(\neg H | H) \leq Pr(F | H) \leq Pr(H | H) = 1$
- $Pr(F \cup G | H) = Pr(F|H) + Pr(G|H) \text{ if } F \cap G = \emptyset$
- $Pr(F \cap G | H) = Pr(G | H) Pr(F | G \cap H)$

*Partitions and probability*

Suppose $\{ H_1, \ldots, H_K\}$ is a partition of $\mathcal{H}$, $Pr(\mathcal{H}) = 1$ and $E$ is some specific event. From the axioms of probability one may prove:

- *Rule of total probability*: 

\begin{equation}
\sum_{k = 1}^K Pr(H_k) = 1
\end{equation}

- *Rule of marginal probability*:

\begin{equation}
\begin{aligned} 
Pr(E) &= \sum_{k = 1}^K Pr(E \cap H_k)\\ &= \sum_{k = 1}^K Pr(E | H_k) Pr(H_k) 
\end{aligned}
\end{equation}

- *Bayes' theorem*: 

\begin{equation}
Pr(H_j | E) = \frac{Pr(E|H_j) Pr(H_j)}{Pr(E)}
\end{equation}

Note it is often useful to replace the denominator, $Pr(E)$, using the rule of marginal probability.

*Independence*

Two events $F$ and $G$ are conditionally independent given $H$ if $Pr(F \cap G |H) = Pr(F|H) Pr(G|H)$.


### Additional related content (Ch2)

- *Law of total expectation* $E(X) = E(E(X|Y))$ 
- *Law of total variance* $Var(X) = E(Var(X|Y) + Var(E(X|Y))$

### Chapter 3

## Definitions and conjugacy

- Be able to define **likelihood**, **prior**, **poserior**, **normalizing constant**

::: callout-note
## Definition
A prior $p(\theta)$ is said to be **conjugate** to the data generative model $p(y|\theta)$ if the family of the posterior is necessarily in the same family as the prior. In math,  $p(\theta)$ is conjugate to $p(y|\theta)$ if

$$
p(\theta) \in \mathcal{P} \implies p(\theta | y) \in \mathcal{P}
$$
:::

- Examples of conjugate models: beta-binomial, gamma-Poisson.

## Reliability 

::: callout-note
## Definition
Let $\Phi$ be the support of $\theta$.
An interval $(l(y), u(y)) \subset \Phi$ has 95% **posterior coverage** if 

$$
p(l(y) < \theta < u(y) | y ) = 0.95
$$

Interpretation: after observing $Y = y$, our probability that $\theta \in (l(y), u(y))$ is 95%.

Such an interval is called 95% posterior confidence interval (CI). It may also sometimes be referred to as a 95% "credible interval" to distinguish it from a frequentist CI.
:::

::: callout-note
## Definition

A $100 \times (1-\alpha)$% **high posterior density** (HPD) region is a set $s(y) \subset \Theta$ such that 


1. $p(\theta \in s(y) | Y = y) = 1 - \alpha$

2. If $\theta_a \in s(y)$ and $\theta_b \not\in s(y)$, then $p(\theta_a | Y = y) > p(\theta_b | Y = y)$
:::

## Exponential families

If density $p(y|\theta)$ can be written $h(y) c(\phi) e^{\phi t(y)}$ for some transform $\phi = f(\theta)$ we can say $p(y|\theta)$ belongs in the exponential family, and the conjugate prior is $p(\phi) =c(\phi)^{n_0} e^{n_0 t_0 \phi}$.