[
  {
    "objectID": "labs/lab0.html",
    "href": "labs/lab0.html",
    "title": "Hello R.",
    "section": "",
    "text": "This ‚Äòlab 0‚Äô will introduce you to the course computing workflow. The main goal of today is to get you setup in RStudio and play around with a few fundamental skills."
  },
  {
    "objectID": "labs/lab0.html#r-and-r-studio",
    "href": "labs/lab0.html#r-and-r-studio",
    "title": "Hello R.",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\nBelow are the components of a Quarto (.qmd) file. Note: this is essentially the same as an Rmarkdown (.Rmd) file, with a couple built-in quality of life additions."
  },
  {
    "objectID": "labs/lab0.html#yaml",
    "href": "labs/lab0.html#yaml",
    "title": "Hello R.",
    "section": "YAML",
    "text": "YAML\nThe top portion of your Quarto or R markdown file (between the three dashed lines) is called YAML. It stands for ‚ÄúYAML Ain‚Äôt Markup Language‚Äù. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nGo to file > new file, Quarto document. Input title ‚ÄúLab 0‚Äù, and change the author name to your name. Select pdf output and press Create. Render the document. Examine the rendered document."
  },
  {
    "objectID": "labs/lab0.html#latex",
    "href": "labs/lab0.html#latex",
    "title": "Hello R.",
    "section": "LaTeX",
    "text": "LaTeX\nAssignments in this course are not required to be written in LaTeX. You may write equations by hand and scan them as a pdf to submit to Gradescope. However, LaTeX is the typesetting system to communicate statistics and mathematics professionally. It‚Äôs worthwhile to use. Moreover, it‚Äôs fully supported within .Rmd and .qmd files.\nIf you‚Äôre using R on your local machine, you may need to install\n\nMiKTeX (if you‚Äôre using windows): https://miktex.org/\nMacTeX (if you‚Äôre using macOS): https://www.tug.org/mactex/\nTeXLive (if you‚Äôre using linux): https://tug.org/texlive/\n\nTo write a LaTeX equation within your markdown document, simply use $$ to surround blocks of math and $ to surround in-line math.\nExample: copy and paste the following and then render.\nWe can see that $\\beta_0 = 2 and \\beta_1 = 3$ is the OLS solution under our model\n\n$$\ny = \\beta_0 + \\beta_1 x\n$$\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is no space between $ and math. Whitespace may cause the document to fail to render.\n\n\nCheck out this LaTeX cheatsheet to typeset a variety of math."
  },
  {
    "objectID": "labs/lab0.html#exercises",
    "href": "labs/lab0.html#exercises",
    "title": "Hello R.",
    "section": "Exercises",
    "text": "Exercises\nThe following exercises are designed to help you gain basic familiarity with R as well as the quirks of floating point arithmetic.\n\nFloating point algebra.\n\nDo floating point numbers obey the rules of algebra? For example, one of the rules of algebra is additive association. (x + y) + z == x + (y + z). Check if this is true in R using \\(x = 0.1\\), \\(y = 0.1\\) and \\(z = 1\\). Explain what you find.\n\nAdditional examples of floating point pecularity are provided below.\n\n# example 1\n0.2 == 0.6 / 3\n# example 2\npoint3 <- c(0.3, 0.4 - 0.1, 0.5 - 0.2, 0.6 - 0.3, 0.7 - 0.4)\npoint3\npoint3 == 0.3\n\nTo work around these issues, you could use all.equal() for checking the equality of two double quantities in R. What does all.equal() do?\n\n# example 1, all.equal()\nall.equal(0.2, 0.6 / 3)\n# example 2, all.equal()\npoint3 <- c(0.3, 0.4 - 0.1, 0.5 - 0.2, 0.6 - 0.3, 0.7 - 0.4)\npoint3\nall.equal(point3, rep(.3, length(point3)))\n\n\nWhat do these functions do?\n\nUse ?rnorm to read the documentation and explain the output of each of the following:\n\nrnorm(10, mean = 1, sd = 2)\npnorm(0)\ndnorm(0.5)\nqnorm(0.5)\n\nHow is dnorm(0.5) computed? Can you compute it manually?\n\nShow it numerically\n\n\\(X \\sim N(\\mu, \\sigma^2)\\) means that \\(X\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Show, using rnorm that if \\(X \\sim N(0, 1)\\) and \\(Y \\sim N(1, 2)\\) that \\(\\mathbb{E}(X + Y) = 1\\) and \\(\\mathbb{V}(X + Y) = 3\\)\n\n\n\n\nControl flow\n\n\n# for loop example\nfor (i in 1:5) {\n  cat(\"Hello\", i, \"\\n\")\n}\n\n# if else example\nx = 1\nif(x > 0) {\n  print(\"I'm positive x is greater than 0.\")\n} else {\n  print(\"I'm not so positive about x being positive\")\n}\n\nAssume there are 50 days of class. Suppose that, on any given day, there is a \\(X_i\\) probability student \\(i\\) will come to class. Every day you come to class, you obtain Y points towards your final grade. Every day that you don‚Äôt come to class, you obtain Z points towards your final grade.\nAssume \\(Y \\sim Uniform(1.9, 2)\\) and \\(Z \\sim Uniform(1, 2)\\).\nAssume student A has a 95% chance of coming to class any given day (X = 0.95) and student B has a 70% of coming to class any given day (X = 0.7). While there are more efficient ways to do this, practice using a for loop, a conditional if statement, rbinom and runif to simulate one possible final grade for each student."
  },
  {
    "objectID": "labs/lab0.html#style-guidelines",
    "href": "labs/lab0.html#style-guidelines",
    "title": "Hello R.",
    "section": "Style guidelines",
    "text": "Style guidelines\nAlthough coding is not the primary focus of this course, there are a short list below of fundamental principles we will follow. Note: some of these stylistic principles may not be followed in the text!\nFirst, it‚Äôs easy to write code that runs off the page when you render to pdf. This happens when you write more than 80 characters in a single line of code. To ensure this doesn‚Äôt happen, make sure your code doesn‚Äôt have 80 characters in a single line. To enable a vertical line in the RStudio IDE that helps you visually see the limit, go to Tools > Global Options > Code > Display > Show margin > 80. This will enable a vertical line in your .qmd files that shows you where the 80 character cutoff is for code chunks. Instructions may vary slightly for local installs of RStudio.\n\nAll binary operators should be surrounded by space. For example x + y is appropriate. x+y is not.\nAny and all pipes %>% or |> as well as ggplot layers + should be followed by a new line.\nYou should be consistent with stylistic choices, e.g.¬†only use 1 of = vs <- and %>% vs |>\nYour name should be at the top (in the YAML) of each document under ‚Äúauthor:‚Äù\n\nIf you have any questions about style, please ask a member of the teaching team."
  },
  {
    "objectID": "chapterSummaries.html",
    "href": "chapterSummaries.html",
    "title": "Chapter summaries",
    "section": "",
    "text": "Axioms of probability\nFor all sets \\(F\\), \\(G\\) and \\(H\\),\n\n\\(0 = Pr(\\neg H | H) \\leq Pr(F | H) \\leq Pr(H | H) = 1\\)\n\\(Pr(F \\cup G | H) = Pr(F|H) + Pr(G|H) \\text{ if } F \\cap G = \\emptyset\\)\n\\(Pr(F \\cap G | H) = Pr(G | H) Pr(F | G \\cap H)\\)\n\nPartitions and probability\nSuppose \\(\\{ H_1, \\ldots, H_K\\}\\) is a partition of \\(\\mathcal{H}\\), \\(Pr(\\mathcal{H}) = 1\\) and \\(E\\) is some specific event. From the axioms of probability one may prove:\n\nRule of total probability:\n\n\\[\\begin{equation}\n\\sum_{k = 1}^K Pr(H_k) = 1\n\\end{equation}\\]\n\nRule of marginal probability:\n\n\\[\\begin{equation}\n\\begin{aligned}\nPr(E) &= \\sum_{k = 1}^K Pr(E \\cap H_k)\\\\ &= \\sum_{k = 1}^K Pr(E | H_k) Pr(H_k)\n\\end{aligned}\n\\end{equation}\\]\n\nBayes‚Äô theorem:\n\n\\[\\begin{equation}\nPr(H_j | E) = \\frac{Pr(E|H_j) Pr(H_j)}{Pr(E)}\n\\end{equation}\\]\nNote it is often useful to replace the denominator, \\(Pr(E)\\), using the rule of marginal probability.\nIndependence\nTwo events \\(F\\) and \\(G\\) are conditionally independent given \\(H\\) if \\(Pr(F \\cap G |H) = Pr(F|H) Pr(G|H)\\).\n\n\n\nLaw of total expectation \\(E(X) = E(E(X|Y))\\)\nLaw of total variance \\(Var(X) = E(Var(X|Y)) + Var(E(X|Y))\\)\n\n\n\n\n\n\n\nNote\n\n\n\nRemember we can always add conditioning statements e.g.\n\nLaw of total expectation \\(E(X|Z) = E(E(X|Y)|Z)\\)\nLaw of total variance \\(Var(X|Z) = E(Var(X|Y)|Z) + Var(E(X|Y)|Z)\\)"
  },
  {
    "objectID": "hw/hw03.html",
    "href": "hw/hw03.html",
    "title": "Homework 3",
    "section": "",
    "text": "Let \\(Y_1, \\ldots Y_n | \\theta\\) be an i.i.d. random sample from a population with pdf \\(p(y|\\theta)\\) where\n\\[\np(y|\\theta) = \\frac{2}{\\Gamma(a)} \\theta^{2a} y^{2a -1} e^{-\\theta^2 y^2}\n\\]\nand \\(y > 0\\), \\(\\theta > 0\\), \\(a > 0\\).\nFor this density,\n\\[\n\\begin{aligned}\nE~Y|\\theta &= \\frac{\\Gamma(a + \\frac{1}{2})}{\\theta \\Gamma(a)}\\\\\nE~Y^2|\\theta &= \\frac{a}{\\theta^2}\n\\end{aligned}\n\\]\nCall this density \\(g^2\\) such that \\(Y_1, \\ldots Y_n | \\theta \\sim g^2(a, \\theta)\\).\n\nFind the joint pdf of \\(Y_1, \\ldots Y_n | \\theta\\) and simplify as much as possible.\nSuppose \\(a\\) is known but \\(\\theta\\) is unknown. Identify a simple conjugate class of priors for \\(\\theta\\). For any arbitrary member of the class, identify the posterior density \\(p(\\theta | y_1, \\ldots y_n)\\).\nObtain a formula for \\(E~ \\theta | Y_1, \\ldots Y_n\\) when the prior is in the conjugate class."
  },
  {
    "objectID": "hw/hw03.html#exercise-2",
    "href": "hw/hw03.html#exercise-2",
    "title": "Homework 3",
    "section": "Exercise 2",
    "text": "Exercise 2\nSuppose \\(Y|\\theta \\sim \\text{binary}(\\theta)\\) and we want to use \\(\\theta \\sim \\text{Uniform}(0, 1)\\) to represent our lack of knowledge about \\(\\theta\\). However, we are interested in the log-odds \\(\\gamma = \\log \\frac{\\theta}{1 - \\theta}\\).\n\nFind the prior distribution for \\(\\gamma\\) induced by our prior on \\(\\theta\\). Is the prior informative about \\(\\gamma\\)? Verify \\(p(\\gamma)\\) using Monte Carlo sampling and then plotting the empirical density of the samples along with the closed-form solution.\nAssume some data come in and \\(y = 7\\) out of \\(n = 10\\) trials. Report the posterior mean and 95% posterior confidence interval for \\(\\gamma\\)."
  },
  {
    "objectID": "hw/hw03.html#exercise-3",
    "href": "hw/hw03.html#exercise-3",
    "title": "Homework 3",
    "section": "Exercise 3",
    "text": "Exercise 3\nSuppose an experimental machine in a lab is either fine, or comes from a bad batch of machines that are to be recalled by the manufacturer. Scientists in the lab want to estimate the failure rate of their machine and decide whether or not to return it. They encode their prior uncertainty about the failure rate \\(\\theta\\) with the following density:\n\\[\np(\\theta) = \\frac{1}{4} \\frac{\\Gamma(10)}{\\Gamma(2)\\Gamma(8)}\\left[\n3 \\theta (1 - \\theta)^7 + \\theta^7(1- \\theta)\n\\right]\n\\]\n\nMake a plot of this prior density and explain why it makes sense for the scientists. Based on the prior density, which do the scientists think is more likely - that their machine is fine, or bad?\nThe scientists run the machine \\(n\\) times. Let \\(y_i\\) be one if the machine fails on the \\(i\\)th run, and zero otherwise. Write out the posterior distribution of \\(\\theta\\) given \\(y_1, \\ldots, y_n\\) (up to a proportionality constant) and simplify as much as possible.\nFor the case that \\(n = 4\\), make a plot of the (unscaled) posterior of \\(\\theta\\) for the five cases \\(\\sum y_i \\in \\{ 0, 1, 2, 3, 4\\}\\).\nThe posterior is a mixture (weighted average) of two distributions that you know. Identify these two distributions, including their parameters."
  },
  {
    "objectID": "hw/hw02.html",
    "href": "hw/hw02.html",
    "title": "Homework 2",
    "section": "",
    "text": "Let \\(Y_1, Y_2 | \\theta\\) be i.i.d. binary(\\(\\theta\\)), so that \\(p(y_1, y_2 | \\theta) = \\theta ^{y_1 + y_2} (1- \\theta) ^{2 - y_1 - y_2}\\) and let \\(\\theta \\sim \\text{beta}(\\eta, \\eta)\\)\n\nCompute \\(E~Y_i\\) and \\(Var~Y_i\\) (the mean and variance of \\(Y_i\\) unconditional on \\(\\theta\\)) as a function of \\(\\eta\\)\nCompute \\(E~Y_1 Y_2\\), which is the same as \\(p(Y_1 = 1, Y_2 = 1)\\) unconditional on \\(\\theta\\). You can do this with help from the formula on page 33 of the book.\nUsing the terms you have calculated above, make a graph of the correlation between \\(Y_1\\) and \\(Y_2\\) as a function of \\(\\eta\\).\nInterpreting \\(\\eta\\) as how confident you are that \\(\\theta\\) is near \\(\\frac{1}{2}\\), and interpreting \\(Cor(Y_1, Y_2)\\) as how much information \\(Y_1\\) and \\(Y_2\\) provide about each other, explain in words why the correlation changes as a function of \\(\\eta\\)."
  },
  {
    "objectID": "hw/hw02.html#exercise-2",
    "href": "hw/hw02.html#exercise-2",
    "title": "Homework 2",
    "section": "Exercise 2",
    "text": "Exercise 2\nSuppose \\(n\\) individuals volunteer to count birds in a forest. Let \\(Y_i\\) be the number of birds counted by individual \\(i\\), and let \\(x_i\\) be the number of hours spent in the forest by volunteer \\(i\\). We will model the data \\(Y_1, \\ldots Y_n\\) as being independent given \\(\\theta\\), but not identically distributed. Specifically, our model is that \\(Y_i | \\theta \\sim \\text{Pois}(\\theta x_i)\\), independently for \\(i = 1, \\ldots n\\).\n\nCompute \\(E~Y_i | \\theta\\) and explain what \\(\\theta\\) represents.\nWrite out a formula for the joint pdf \\(p(y_1, \\ldots y_n |\\theta)\\) and simplify as much as possible. Find the MLE, that is, the value of \\(\\theta\\) that maximizes \\(p(y_1, \\ldots y_n | \\theta)\\). Explain why it makes sense.\nLet \\(\\theta \\sim \\text{gamma}(a, b)\\). Find a formula for the posterior mode of \\(\\theta\\) and compare to the MLE."
  },
  {
    "objectID": "hw/hw02.html#exercise-3",
    "href": "hw/hw02.html#exercise-3",
    "title": "Homework 2",
    "section": "Exercise 3",
    "text": "Exercise 3\nLet \\(\\theta_1\\) be the prevalence of a rare allele among people with Alzheimer‚Äôs disease, and let \\(\\theta_2\\) be the prevalence among people without the disease. To estimate \\(\\theta_1\\) and \\(\\theta_2\\), a sample of \\(n_1 = 19\\) Alzheimer‚Äôs patients and \\(n_2 = 176\\) control subjects are genotyped for the presence of the allele. Let \\(Y_1\\) and \\(Y_2\\) be the number of people in the two samples who have the allele. We will model \\(Y_1\\) and \\(Y_2\\) as independent with \\(Y_1 | \\theta_1 \\sim \\text{binomial}(n_1, \\theta_1)\\) and \\(Y_2 | \\theta_2 \\sim \\text{binomial}(n_2, \\theta_2)\\). Prior studies suggest that \\(\\theta_2 \\sim \\text{beta}(2, 30)\\) is a reasonable prior distribution for \\(\\theta_2\\). For now, we will use the same prior distribution for \\(\\theta_1\\). The study is performed and the data are that \\(Y_1 = 1\\) and \\(Y_2 = 16\\).\n\nState the posterior distributions of \\(\\theta_1\\) and \\(\\theta_2\\). Plot the posterior densities together on a single graph with the prior density, and compare all three curves with words.\nCompute the posterior mean and a 95% posterior interval for each of \\(\\theta_1\\) and \\(\\theta_2\\).\nWith a picture, with words, or mathematically, try to describe different kind of joint prior distribution for \\(\\theta_1\\) and \\(\\theta_2\\) that represents the \\(\\theta_1\\) and \\(\\theta_2\\) are close to each other, but highly uncertain."
  },
  {
    "objectID": "hw/hw00.html",
    "href": "hw/hw00.html",
    "title": "Homework 0",
    "section": "",
    "text": "This math assessment is meant to help both you and the instructor identify gaps in background knowledge both at the class and individual level."
  },
  {
    "objectID": "hw/hw00.html#exercise-1",
    "href": "hw/hw00.html#exercise-1",
    "title": "Homework 0",
    "section": "Exercise 1",
    "text": "Exercise 1\nSimplify\n\\[\n\\log(e^{a_1} e^{a_2} e^{a_3} \\cdots e^{a_n})\n\\]"
  },
  {
    "objectID": "hw/hw00.html#exercise-2",
    "href": "hw/hw00.html#exercise-2",
    "title": "Homework 0",
    "section": "Exercise 2",
    "text": "Exercise 2\nFind the derivative.\n\\[\n\\frac{d}{dx} \\left( \\frac{x}{\\log x} \\right)\n\\]"
  },
  {
    "objectID": "hw/hw00.html#exercise-3",
    "href": "hw/hw00.html#exercise-3",
    "title": "Homework 0",
    "section": "Exercise 3",
    "text": "Exercise 3\nWhat is the ordinary least squares estimator of \\(\\beta\\) (1-dimensional) in the linear regression \\(y = x \\beta + \\epsilon\\) with iid errors?"
  },
  {
    "objectID": "hw/hw00.html#exercise-4",
    "href": "hw/hw00.html#exercise-4",
    "title": "Homework 0",
    "section": "Exercise 4",
    "text": "Exercise 4\nWhat is the ordinary least squares estimator of \\(\\beta\\) (p-dimensional) in the linear regression \\(y = X \\beta + \\epsilon\\) with iid errors?"
  },
  {
    "objectID": "hw/hw00.html#exercise-5",
    "href": "hw/hw00.html#exercise-5",
    "title": "Homework 0",
    "section": "Exercise 5",
    "text": "Exercise 5\nIn linear regression with p-dimensional Œ≤, what is the interpretation of the estimate for the jth coefficient?"
  },
  {
    "objectID": "hw/hw00.html#exercise-6",
    "href": "hw/hw00.html#exercise-6",
    "title": "Homework 0",
    "section": "Exercise 6",
    "text": "Exercise 6\nCompute the integral,\n\\[\n\\int_{-\\infty}^{\\infty} e^{-x^2} dx\n\\]"
  },
  {
    "objectID": "hw/hw00.html#exercise-7",
    "href": "hw/hw00.html#exercise-7",
    "title": "Homework 0",
    "section": "Exercise 7",
    "text": "Exercise 7\n\\(X \\sim N(\\mu, \\sigma^2)\\) reads ‚ÄúX is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nLet\n\\[\n\\begin{aligned}\nX &\\sim N(0, 1),\\\\\nY &\\sim N(3, 2),\\\\\nZ &= X + Y\n\\end{aligned}\n\\]\nWhat is the distribution of \\(Z\\)? What is \\(\\mathbb{E}[Z]\\) and \\(Var(Z)\\)?"
  },
  {
    "objectID": "hw/hw00.html#exercise-8",
    "href": "hw/hw00.html#exercise-8",
    "title": "Homework 0",
    "section": "Exercise 8",
    "text": "Exercise 8\nIn your own words, the ‚Äúsupport‚Äù of random variable is‚Ä¶"
  },
  {
    "objectID": "hw/hw00.html#exercise-9",
    "href": "hw/hw00.html#exercise-9",
    "title": "Homework 0",
    "section": "Exercise 9",
    "text": "Exercise 9\nTRUE/FALSE: The product of two uniform[0, 1] random variables is uniform[0, 1]."
  },
  {
    "objectID": "hw/hw01.html",
    "href": "hw/hw01.html",
    "title": "Homework 1",
    "section": "",
    "text": "Let \\(X_i \\in \\mathcal{X}\\) for all \\(i \\in \\{1, 2, \\ldots\\}\\) and suppose our belief model for \\(\\mathbf{X} = \\{ X_1, \\ldots X_n \\}\\) is exchangeable for all \\(n\\). Show, using de Finetti‚Äôs theorem, that for all \\(i \\neq j\\),\n\\[\nCov(X_i, X_j) \\geq 0 \\text{ and }\n\\]\n\\[\nCorr(X_i, X_j) \\geq 0.\n\\]"
  },
  {
    "objectID": "hw/hw01.html#exercise-2",
    "href": "hw/hw01.html#exercise-2",
    "title": "Homework 1",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n2.2 from Hoff"
  },
  {
    "objectID": "hw/hw01.html#exercise-3",
    "href": "hw/hw01.html#exercise-3",
    "title": "Homework 1",
    "section": "Exercise 3",
    "text": "Exercise 3\n\n2.3 from Hoff"
  },
  {
    "objectID": "hw/hw01.html#exercise-4",
    "href": "hw/hw01.html#exercise-4",
    "title": "Homework 1",
    "section": "Exercise 4",
    "text": "Exercise 4\n\n2.6 from Hoff"
  },
  {
    "objectID": "solutions/hw01s.html",
    "href": "solutions/hw01s.html",
    "title": "Homework 1 solutions",
    "section": "",
    "text": "By definition of covariance,\n\\[\nCov(X_i, X_j) = \\mathbb{E}~X_i X_j - \\mathbb{E}~X_i ~\\mathbb{E}~X_j\n\\]\nBy the law of total expectation,\n\\[\n= \\mathbb{E}~[\\mathbb{E}~X_i X_j| \\theta] -\n\\mathbb{E}~[\\mathbb{E}~X_i | \\theta]\n\\mathbb{E}~[\\mathbb{E}~X_j | \\theta]\n\\]\nBy de Finetti‚Äôs theorem, exchangeability of \\(X_i\\) and \\(X_j\\) implies the two variables are conditionally iid relative to \\(\\theta\\). Therefore,\n\\[\n\\begin{aligned}\n\\mathbb{E}~[\\mathbb{E} ~ X_i X_j| \\theta] &= \\mathbb{E}~\n\\left[\n\\int \\int x_i~x_j~p(x_i, x_j |\\theta) dx_i~dx_j\n\\right]\\\\\n&=\\mathbb{E}~\n\\left[\n\\int x_i~p(x_i |\\theta) dx_i \\cdot \\int x_j~p(x_j |\\theta) dx_j\n\\right]\\\\\n&=\n\\mathbb{E}\n\\left[\n\\mathbb{E}~ X_i | \\theta \\cdot\n\\mathbb{E}~ X_j | \\theta\n\\right]\n\\end{aligned}\n\\]\nSimilarly,\n\\[\n\\mathbb{E}~\nX_i | \\theta\n=\n\\mathbb{E}\nX_j | \\theta\n\\]\nso that in total,\n\\[\nCov(X_i, X_j) =\n\\mathbb{E}~\n\\left[\n\\left(\n\\mathbb{E}~X_i | \\theta\n\\right)^2\n\\right]\n-\n\\left(\n\\mathbb{E}~\n\\left[\n\\mathbb{E}~\nX_i | \\theta\n\\right]\n\\right)^2\n\\]\nNote that \\(\\mathbb{E}~X_i | \\theta\\) is some function of \\(\\theta\\), say \\(g(\\theta)\\). It is easy to see\n\\[\nCov(X_i, X_j) = Var(g(\\theta)) \\geq 0\n\\]\n\nExplicit detail:\nBy de Finetti‚Äôs theorem exchangeability of \\(X_i\\) and \\(X_j\\) implies\n\\[\np(x_i, x_j) = \\int p(x_i | \\theta) p(x_j | \\theta) p(\\theta) d\\theta\n\\]\nRecall \\(p(x_i, x_j) = \\int p(x_i, x_j, \\theta) d\\theta\\) and therefore \\(p(x_i, x_j) = \\int p(x_i, x_j | \\theta) p(\\theta) d\\theta\\). By comparison to the above, notice\n\\[\np(x_i, x_j | \\theta) = p(x_i | \\theta) p(x_j|\\theta).\n\\]\nIn words, \\(x_i\\) and \\(x_j\\) are conditionally independent given \\(\\theta\\)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian methods and modern statistics",
    "section": "",
    "text": "Week\nDate\nTopic\nReading\nNotes\nAssignment\n\n\n\n\n1\nMon Aug 28\nlab: welcome\n\nüíª\nhello R\n\n\n\nTue Aug 29\nintro, history, notation\nCh. 2\n\nhw 0\n\n\n\nThu Aug 31\nprobability, exchangeability\nCh. 2\nüíª\nhw 1\n\n\n2\nMon Sep 04\nNO CLASS\n\n\n\n\n\n\nTue Sep 05\nsingle parameter estimation\nCh. 3\nüíª\n\n\n\n\nThu Sep 07\nPoisson model and conjugacy\nCh. 3\n\nhw 2\n\n\n3\nMon Sep 11\nlab: MLE and MAP estimator\n\nüíª\n\n\n\n\nTue Sep 12\nreliability, exp. families\nCh. 3\nüíª, üìù\n\n\n\n\nThu Sep 14\nprediction, Monte Carlo intro\nCh. 4\nüíª, üìù\nhw 3\n\n\n4\nMon Sep 18\nlab: prior sensitivity and change of variables\n\nüíª\n\n\n\n\nTue Sep 19\nMonte Carlo integration\nCh. 4\nüíª\n\n\n\n\nThu Sep 21\nthe normal model\nCh. 5\nüíª\n\n\n\n5\nMon Sep 25\npractice and review\n\nüíª\n\n\n\n\nTue Sep 26\ncatch up / review\n\n\n\n\n\n\nThu Sep 28\nExam I\n\n\n\n\n\n6\nMon Oct 02\nNO LAB\n\n\n\n\n\n\nTue Oct 03\nthe normal model II\nCh. 5\n\nhw 4\n\n\n\nThu Oct 05\nGibbs sampling\nCh. 6\n\n\n\n\n7\nMon Oct 09\n\n\n\n\n\n\n\nTue Oct 10\nMCMC diagnostics\nCh. 6\n\nhw 5\n\n\n\nThu Oct 12\nmultivariate normal (mvn)\nCh. 7\n\n\n\n\n8\nMon Oct 16\nNO CLASS\n\n\n\n\n\n\nTue Oct 17\nNO CLASS\n\n\n\n\n\n\nThu Oct 19\nmvn parameter estimation\nCh. 7\n\nhw 6\n\n\n9\nMon Oct 23\n\n\n\n\n\n\n\nTue Oct 24\nhierarchical modeling intro\nCh. 8\n\n\n\n\n\nThu Oct 26\nhierarchical examples\nCh. 8\n\nhw 7\n\n\n10\nMon Oct 30\n\n\n\n\n\n\n\nTue Oct 31\nBayesian regression I\nCh. 9\n\n\n\n\n\nThu Nov 02\nBayesian regression II\nCh. 9\n\nhw 8\n\n\n11\nMon Nov 06\n\n\n\n\n\n\n\nTue Nov 07\nmodel selection\nCh. 9\n\n\n\n\n\nThu Nov 09\n\n\n\n\n\n\n12\nMon Nov 13\n\n\n\n\n\n\n\nTue Nov 14\ncatch up / review\n\n\n\n\n\n\nThu Nov 16\nExam II\n\n\n\n\n\n13\nMon Nov 20\n\n\n\n\n\n\n\nTue Nov 21\n\n\n\n\n\n\n\nThu Nov 23\nNO CLASS\n\n\n\n\n\n14\nMon Nov 27\n\n\n\n\n\n\n\nTue Nov 28\n\n\n\nhw 9\n\n\n\nThu Nov 30\n\n\n\n\n\n\n15\nMon Dec 04\n\n\n\n\n\n\n\nTue Dec 05\n\n\n\n\n\n\n\nThu Dec 07"
  },
  {
    "objectID": "slides/lab1.html#example-normal-likelihood",
    "href": "slides/lab1.html#example-normal-likelihood",
    "title": "MLEs and MAPs",
    "section": "Example: normal likelihood",
    "text": "Example: normal likelihood\nLet \\(X\\) be the resting heart rate (RHR) in beats per minute of a student in this class.\nAssume RHR is normally distributed with some mean \\(\\mu\\) and standard deviation \\(8\\).\n\n\\[\n\\textbf{Data-generative model: } X_i \\overset{\\mathrm{iid}}{\\sim} N(\\mu, 64)\n\\]\n\n\nIf we observe three student heart rates, {75, 58, 68} then our likelihood\n\\[L(\\mu) = f_x(75 |\\mu) \\cdot f_x(58|\\mu) \\cdot f_x(68|\\mu).\\]\nThat is, the joint density function of the observed data, viewed as a function of the parameter.\n\n\n\n\n\n\n\n\nImportant\n\n\nThe likelihood itself is not a density function. The integral with respect to the parameter does not need to equal 1."
  },
  {
    "objectID": "slides/lab1.html#visualizing-the-likelihood",
    "href": "slides/lab1.html#visualizing-the-likelihood",
    "title": "MLEs and MAPs",
    "section": "Visualizing the likelihood",
    "text": "Visualizing the likelihood\n\\[L(\\mu) = f_x(75 |\\mu) \\cdot f_x(58|\\mu) \\cdot f_x(68|\\mu).\\]\n\ndatalikelihood functionplotplot code\n\n\n\nx = c(75, 58, 68)\n\n\n\n\nL = function(mu, x) {\n  stopifnot(is.numeric(x))\n  n = length(x)\n  likelihood = 1\n  for(i in 1:n){\n    likelihood = likelihood * dnorm(x[i], mean = mu, sd = 8)\n  }\n  return(likelihood)\n}\n\n\n\n\n\n\n\n\n\n\n\nggplot() +\n  xlim(c(50, 83)) +\n  geom_function(fun = L, args = list(x = x)) +\n  theme_bw() +\n  labs(x = expression(mu), y = \"likelihood\") + \n  geom_vline(xintercept = 67, color = 'red')\n\n\n\n\n\nThe maximum likelihood estimate \\(\\hat{\\mu} = \\frac{75 + 58 + 68}{3} = 67\\).\nThe maximum likelihood estimate is the parameter value that maximizes the likelihood function."
  },
  {
    "objectID": "slides/lab1.html#the-log-likelihood",
    "href": "slides/lab1.html#the-log-likelihood",
    "title": "MLEs and MAPs",
    "section": "The log-likelihood",
    "text": "The log-likelihood\nNotice how small the y-axis is on the previous slide. What happens to the scale of the likelihood as we add additional data points?\n\\[\nL(\\mu) = \\prod_{i = 1}^{n} f_x(x_i |\\mu)\n\\]\n\nSince densities often evaluate between 0 and 1, multiplying many together (as we usually do in likelihoods) can quickly result in floating point underflow. That is, numbers smaller than the computer can actually represent in memory.\n\nNote: sometimes densities evaluate to greater than 1 (e.g.¬†dnorm(0, 0, 0.001)) and multiplying several together can result in overflow.\n\n\n\nlog to the rescue!\n\nlog is a monotonic function, i.e.¬†\\(x > y\\) implies \\(\\log(x) > \\log(y)\\), because of this the maximum of \\(f\\) is the same as the maximum of \\(\\log f\\).\nadditionally, log turns products into sums\n\nin practice, we always work with the log-likelihood,\n\\[\n\\log L(\\mu) = \\sum_{i = 1}^n \\log f_x(x_i | \\mu).\n\\]"
  },
  {
    "objectID": "slides/lab1.html#maximum-likelihood-estimation-mle",
    "href": "slides/lab1.html#maximum-likelihood-estimation-mle",
    "title": "MLEs and MAPs",
    "section": "Maximum likelihood estimation (MLE)",
    "text": "Maximum likelihood estimation (MLE)\nHow did we know to take the average of the values to find the maximum likelihood estimator \\(\\hat{\\mu}\\)?\n\nFrom calculus, we know that to maximize a function, we need to find where the slope equals zero (technically, to ensure we find some maxima and not a minima we need to also check that the second derivative is negative).\nExample: normal likelihood\nFor the normal likelihood example on the previous slide, we can see visually that the function is concave.\nTo find the maximum,\n\\[\n\\begin{aligned}\n\\frac{d}{d\\mu} \\log L(\\mu) &= \\sum_{i}\\frac{d}{d\\mu} \\log f_x(x_i |\\mu)\\\\\n&= \\sum_{i}\\frac{d}{d\\mu} \\left[ -\\frac{1}{2} \\log (2 \\pi \\sigma^2) - \\frac{1}{2\\sigma^2} (x_i - \\mu)^2 \\right]\\\\\n&= \\sum_i \\frac{1}{\\sigma^2} (x_i - \\mu)\n\\end{aligned}\n\\]\nSetting the derivative equal to zero,\n\\[\n\\begin{aligned}\n\\sum_i \\left[ x_i - \\hat{\\mu} \\right] &= 0\\\\\nn \\hat{\\mu} &= \\sum_i x_i\\\\\n\\hat{\\mu} &= \\bar{x}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lab1.html#maximum-a-posteriori-probability-map",
    "href": "slides/lab1.html#maximum-a-posteriori-probability-map",
    "title": "MLEs and MAPs",
    "section": "Maximum a posteriori probability (MAP)",
    "text": "Maximum a posteriori probability (MAP)\nIn Bayesian inference, we wish to find the mode of the posterior, not the likelihood.\nTo find the posterior mode, \\(\\hat{\\theta}\\), we instead take the derivative of the log-posterior,\n\\[\n\\frac{d}{d\\theta} \\log p(\\theta | y) = 0\n\\]\nPractice exercise\nAs in class, let\n\\[\nY | \\theta \\sim \\text{binomial}(n, \\theta)\\\\\n\\theta \\sim \\text{beta}(a, b)\n\\]\n\nFind the closed-form solution for the posterior mode \\(\\hat{\\theta}\\).\nRecreate Figure 1 from class using the same data flips provided below but change the prior to \\(\\theta \\sim \\text{beta}(2, 2)\\).\n\n\nset.seed(3)\nflips = rbinom(5000, size = 1, prob = 0.25)\n\n\nAdd a red vertical line to each subplot that shows the MAP estimate under the prior \\(\\theta \\sim \\text{beta}(2, 2)\\).\n\n\n\nüîó sta360-fa23.github.io"
  },
  {
    "objectID": "slides/lab0-welcome.html#introductions",
    "href": "slides/lab0-welcome.html#introductions",
    "title": "Welcome to Lab",
    "section": "Introductions",
    "text": "Introductions\n\n\n\n\nMeet the TA!\nIntroduce yourself (icebreaker)\nFollow along these slides on the course website (under slides): sta360-fa23.github.io\nBookmark this! It‚Äôs the course website."
  },
  {
    "objectID": "slides/lab0-welcome.html#what-to-expect-in-labs",
    "href": "slides/lab0-welcome.html#what-to-expect-in-labs",
    "title": "Welcome to Lab",
    "section": "What to expect in labs",
    "text": "What to expect in labs\n\nDiscussion\nPractice problems\nAssistance on computing portion of homeworks"
  },
  {
    "objectID": "slides/lab0-welcome.html#tips",
    "href": "slides/lab0-welcome.html#tips",
    "title": "Welcome to Lab",
    "section": "Tips",
    "text": "Tips\n\nShow up.\nMake use of office hours. Before you need help!"
  },
  {
    "objectID": "slides/lab0-welcome.html#beginnings",
    "href": "slides/lab0-welcome.html#beginnings",
    "title": "Welcome to Lab",
    "section": "Beginnings",
    "text": "Beginnings\nWhile this is not a computing class, computers are the workhorse of Bayesian statistics and we will use R to both enhance understanding of fundamental course material as well as to implement models to learn about real data sets."
  },
  {
    "objectID": "slides/lab0-welcome.html#set-up-rstudio",
    "href": "slides/lab0-welcome.html#set-up-rstudio",
    "title": "Welcome to Lab",
    "section": "Set up RStudio",
    "text": "Set up RStudio\nOption 1 (easiest): RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers and login with your Duke NetID and Password.\nClick RStudio to log into the Docker container. You should now see the RStudio environment.\n\nIf you haven‚Äôt previously done so, you will need to reserve a container for RStudio first."
  },
  {
    "objectID": "slides/lab0-welcome.html#set-up-rstudio-1",
    "href": "slides/lab0-welcome.html#set-up-rstudio-1",
    "title": "Welcome to Lab",
    "section": "Set up RStudio",
    "text": "Set up RStudio\nOption 2: RStudio on your computer\n\nDownload R from http://www.r-project.org/.\nDownload RStudio, the popular IDE for R, from https://posit.co/downloads/.\n(optionally) download Quarto from https://quarto.org/docs/get-started/"
  },
  {
    "objectID": "slides/lab0-welcome.html#demo",
    "href": "slides/lab0-welcome.html#demo",
    "title": "Welcome to Lab",
    "section": "Demo",
    "text": "Demo\nNext, check your familarity with R/RStudio fundamentals here. You can also find a link to this from the course schedule under ‚ÄúAssignment‚Äù.\n\n\nüîó sta360-fa23.github.io"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links",
    "section": "",
    "text": "RStudio containers\n\nResources\n\nSakai website\n\nlectures and solutions uploaded here under ‚ÄúResources‚Äù tab on the left hand side\n\nGradescope\n\nTextbook\n\nA First Course in Bayesian Statistical Methods by Peter Hoff\nErrata to the textbook"
  },
  {
    "objectID": "notes/estimation1.html",
    "href": "notes/estimation1.html",
    "title": "Is this a fair coin?",
    "section": "",
    "text": "outputcode\n\n\n\n\n [1] 0 1 0 0 0 0 0 0 0 0\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(latex2exp)\nlibrary(glue)\nlibrary(patchwork)\n\nset.seed(3)\nflips = rbinom(5000, size = 1, prob = 0.25)\nflips %>% head(10)\nWe observe 10 flips from the same coin above, where 0 is ‚Äútails‚Äù and 1 is ‚Äúheads‚Äù. In summary, we see Y = 1 heads in 10 coin flips. Is this a fair coin?\nTo articulate this mathematically, let \\(\\theta \\in [0, 1]\\) be the bias-weighting (the chance of heads) of the coin. Fundamentally, we want \\(p(\\theta | y)\\), which we can expand via Bayes‚Äô rule,\n\\[\np(\\theta | y) = \\frac{p(y|\\theta) p(\\theta)}{\\int_{\\theta \\in \\Theta} p(y|\\theta) p(\\theta) d\\theta}\n\\]\nLikelihood: the data generative process. The joint probability (or density) of the data given the parameters of the model. Most often thought of as a function of the parameter. Note: not a density of the parameter.\nPrior: Our a priori (beforehand) beliefs about the true population characteristics.\nPosterior: Our a posteriori (afterwards) beliefs about the true population characteristics after having observed the data set \\(y\\).\nNormalizing constant: A number that enables a pmf or pdf to integrate to 1."
  },
  {
    "objectID": "notes/estimation1.html#uniform-prior",
    "href": "notes/estimation1.html#uniform-prior",
    "title": "Is this a fair coin?",
    "section": "Uniform prior",
    "text": "Uniform prior\nLet \\(y\\) be the number of heads in \\(n\\) coin flips.\n\\[\np(\\theta | y) \\propto \\theta^{y}(1-\\theta)^{n-y}\n\\]\nThis is the kernel of a ___ density, where \\(\\alpha = y + 1\\) and \\(\\beta = n - y + 1\\), hence\n\\[\np(\\theta | y) = \\frac{\\Gamma(n + 2)}{\\Gamma(y + 1)\\Gamma(n-y+1)} \\theta^{y}(1-\\theta)^{n-y}\n\\]\nand the posterior mean is \\(\\frac{y + 1}{n + 2}\\) and the posterior variance is \\(\\frac{(y+1)(n - y + 1)}{(n + 2)^2 (n + 1)}\\).\nLet‚Äôs examine how the posterior evolves with each successive coin flip.\n\nplotscode\n\n\n\n\n\n\n\n\n\n\nN = c(0, 1, 2, 3, 4, 10, 100, 1000, 5000)\n\nfor (i in seq_along(N)) {\nn = N[i]\nif(n == 0) {\n  y = 0\n}\nelse {\n  y = sum(flips[1:n])\n}\n\nx = 0:1 # range\ndf = data.frame(x)\nassign(paste0(\"p\", i),\n  df %>%\n  ggplot(aes(x = x)) +\n  stat_function(fun=dbeta, \n                args = list(shape1 = y + 1, shape2 = n - y + 1)) +\n  labs(y = TeX(\"$p(\\\\theta | y)$\"), x = TeX(\"$\\\\theta$\"),\n       title = glue(\"n = {n}\")) +\n  theme_bw()\n)\n}\n\n(p1 + p2 + p3) / \n  (p4 + p5 + p6) / \n  (p7 + p8 + p9) +\n  plot_annotation(title = \"Figure 1\")"
  },
  {
    "objectID": "notes/estimation1.html#conjugacy",
    "href": "notes/estimation1.html#conjugacy",
    "title": "Is this a fair coin?",
    "section": "Conjugacy",
    "text": "Conjugacy\nIf \\(\\theta \\sim\\) Uniform(0, 1) then \\(p(\\theta)\\) = 1 for all \\(\\theta \\in [0, 1]\\).\nSimilarly, if \\(\\theta \\sim\\) beta(1, 1), then \\(p(\\theta) = 1\\).\nClaim:\nIf\n\\[\n\\begin{aligned}\n\\theta &\\sim \\text{beta}(a, b)\\\\\nY | \\theta &\\sim \\text{binomial}(n, \\theta)\n\\end{aligned}\n\\]\nthen\n\\[\np(\\theta | Y) = \\text{beta}(y + a, n - y + b)\n\\]\n\n\n\n\n\n\nDefinition\n\n\n\nA prior \\(p(\\theta)\\) is said to be conjugate to the data generative model \\(p(y|\\theta)\\) if the family of the posterior is necessarily in the same family as the prior. In math, \\(p(\\theta)\\) is conjugate to \\(p(y|\\theta)\\) if\n\\[\np(\\theta) \\in \\mathcal{P} \\implies p(\\theta | y) \\in \\mathcal{P}\n\\]\n\n\nWhile conjugate priors make calculation easy, they may not accurately reflect our prior beliefs.\n\n\n\n\n\n\nExercise\n\n\n\nProve the claim above."
  },
  {
    "objectID": "notes/estimation1.html#other-priors",
    "href": "notes/estimation1.html#other-priors",
    "title": "Is this a fair coin?",
    "section": "Other priors",
    "text": "Other priors\nIncidentally, people are often satisfied with the choice of likelihood but are worried about the choice of prior.\nLet‚Äôs examine the effect of another couple of priors.\nGiven the coin‚Äôs dubious origin, we might believe a priori that the coin is biased. How could we represent this belief?\n\\[\n\\theta \\sim \\text{beta}(.5, .5)\n\\]\nOr alternatively, we might be strongly believe a priori that the coin is fair. How could we represent this belief?\n\\[\n\\theta \\sim \\text{beta}(20, 20)\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nHow would you update the code of the previous example to show posterior inference under the prior \\(\\theta \\sim\\) beta(2,3)?"
  },
  {
    "objectID": "notes/estimation1.html#prior-data",
    "href": "notes/estimation1.html#prior-data",
    "title": "Is this a fair coin?",
    "section": "Prior data",
    "text": "Prior data\nIn the example above, the parameters, a and b, of the conjugate prior are often thought of as prior data.\n\na: ‚Äúprior number of 1s‚Äù\nb: ‚Äúprior number of 0s‚Äù\na + b: ‚Äúprior sample size‚Äù\n\n\n\n\n\n\n\nExercise\n\n\n\nWe saw above that when a = 20 and b = 20, we needed more data to move the posterior.\nShow that the posterior mean, \\(E(\\theta | y) = \\frac{a + y}{a + b + n}\\) converges to the sample average as \\(n \\rightarrow \\infty\\)."
  },
  {
    "objectID": "notes/probability.html",
    "href": "notes/probability.html",
    "title": "Probability",
    "section": "",
    "text": "This is foundational material. Most of it is background you will have learned in STA230/240. While dry, we must soldier on to get to the exciting stuff."
  },
  {
    "objectID": "notes/probability.html#review-set-theory",
    "href": "notes/probability.html#review-set-theory",
    "title": "Probability",
    "section": "Review: set theory",
    "text": "Review: set theory\n\n\n\n\n\n\nDefinition\n\n\n\nset: a collection of elements, denoted by {}\nExamples\n\n\\(\\phi\\) = {} ‚Äúthe empty set‚Äù\nA = {1, 2, 3}\nB = {taken STA199, has not taken STA199}\nC = {{1,2,3}, {4, 5}}\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nsubset: denoted by \\(\\subset\\), \\(A \\subset B\\) iff \\(a \\in A \\implies a \\in B\\)\nExamples\nUsing the previously examples of A, B and C above,\n\n\\(A \\subset C\\)\n\\(A \\not\\subset B\\)\n\n\n\nRecall:\n\n\\(\\cup\\) means ‚Äúunion‚Äù, ‚Äúor‚Äù\n\\(\\cap\\) means ‚Äúintersection‚Äù, ‚Äúand‚Äù\n\n\n\n\n\n\n\nDefinition\n\n\n\npartition: {\\(H_1, H_2, ... H_n\\)} = \\(\\{H_i\\}_{i = 1}^n\\) is a partition of \\(\\mathcal{H}\\) if\n\nthe union of sets is \\(\\mathcal{H}\\) i.e. \\(\\cup_{i = 1}^n H_i = \\mathcal{H}\\)\nthe sets are disjoint i.e.¬†\\(H_i \\cap H_j = \\phi\\) for all \\(i \\neq j\\)\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nsample space: \\(\\mathcal{H}\\), the set of all possible data sets (outcomes)\nevent: a set of one or more outcomes\nNote: p(\\(\\mathcal{H}\\)) = 1\nExamples\n\nRoll a six-sided die once. The sample space \\(\\mathcal{H} = \\{1, 2, 3, 4, 5, 6\\}\\).\nLet \\(A\\) be the event that the die lands on an even number. \\(A = \\{2, 4, 6 \\}\\)"
  },
  {
    "objectID": "notes/probability.html#axioms-of-probability-in-words",
    "href": "notes/probability.html#axioms-of-probability-in-words",
    "title": "Probability",
    "section": "Axioms of probability (in words)",
    "text": "Axioms of probability (in words)\nP1. Probabilities are between 0 and 1, importantly p(\\(\\neg\\)H|H) = 0 and p(H|H) = 1.\nP2. If two events A and B are disjoint, then p(A or B) = p(A) + p(B)\nP3. The joint probability of two events may be broken down stepwise: p(A,B) = p(A|B)p(B)\n‚Äì\nIt follows that\n\nfor any partition \\(\\{H_i\\}_{i = 1}^n\\), \\(\\sum_{i=1}^n p(H_i) = 1\\) (rule of total probability)\n\nnote: simplest partition \\(p(A) + p(\\neg A) = 1\\)\n\n\\(p(A) = \\sum_{i=1}^n p(A, H_i)\\) (rule of marginal probability)\n\nnote: P3 implies that equivalently, \\(p(A) = \\sum_{i=1}^n p(A | H_i) p(H_i)\\)\n\np(A|B) = p(A,B) / p(B) when p(B) \\(\\neq 0\\)\n\nnote: these statements can also be made where each term is additionally conditioned on another event C\n\n\n\n\n\n\n\n\nExercise\n\n\n\nDerive Bayes‚Äô rule:\n\\(p(H_i|X) = \\frac{p(X|H_i)p(H_i)}{\\sum_k p(X|H_k)p(H_k)}\\)\nusing the axioms of probability.\n\n\nBayes‚Äô rule tells us how to update beliefs about \\(\\{H_i \\}_{i = 1}^n\\) given data \\(X\\)."
  },
  {
    "objectID": "notes/probability.html#independence",
    "href": "notes/probability.html#independence",
    "title": "Probability",
    "section": "Independence",
    "text": "Independence\n\n\n\n\n\n\nDefinition\n\n\n\nTwo events \\(F\\) and \\(G\\) are conditionally independent given \\(H\\) if \\(p(F, G | H) = p(F | H) p(G | H)\\)\n\n\n\n\n\n\n\n\nExercise\n\n\n\nShow conditional independence implies\n\\(p(F | H, G) = p(F | H)\\)\n\n\nThis means that if we know H, then G does not supply any additional information about F."
  },
  {
    "objectID": "notes/probability.html#random-variables",
    "href": "notes/probability.html#random-variables",
    "title": "Probability",
    "section": "Random variables",
    "text": "Random variables\n\n\n\n\n\n\nDefinition\n\n\n\nIn Bayesian inference, a random variable is an unknown numerical quantity about which we make probability statements.\nExamples\n\nData. E.g. the amount of a wheat a field will yield later this year. Since this data has not yet been generated, the quantity is unknown.\nA population parameter. E.g. the true mean resting heart rate of Duke students. Note: this is a fixed (non-random) quantity, but it is also unknown. We use probability to describe our uncertainty in this quantity.\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\ndiscrete random variable: a random variable that takes countably many values. Y is discrete if its possible outcomes can be enumerated \\(\\mathcal{Y} = \\{y_1, y_2, \\ldots \\}\\).\nNote: discrete does not mean finite. There may be infinitely many outcomes!\nExamples\n\nY = the number of children of a randomly sampled person\nY = the number of visible stars in the sky on a randomly sampled night of the year\n\nFor each \\(y \\in \\mathcal{Y}\\), let p(Y) = probability(Y = y). Then p is the probability mass function (pmf) of the random variable Y.\nExamples\n\nBinomial pmf: the probability of \\(y\\) successes in \\(n\\) trials, where each trial has an individual probability of success \\(\\theta\\).\n\\[p(y | \\theta) = {n \\choose y} \\theta ^y (1-\\theta)^y \\text{ for } y \\in \\{0, 1, \\ldots n \\}\\]\n\nsupport: \\(y \\in \\{0, 1, 2, \\ldots n\\}\\)\nsuccess probability \\(\\theta \\in [0, 1]\\)\ndbinom(y, n, theta) computes this pmf in R\n\nPoisson pmf: probability of \\(y\\) events occurring during a fixed interval at a mean rate \\(\\theta\\)\n\\[p(y | \\theta) = \\frac{\\theta^y e^{-\\theta}}{y!}\\]\n\nsupport: \\(y \\in \\{0, 1, 2, \\ldots \\}\\)\nrate \\(\\theta \\in \\mathbb{R}^+\\)\ndpois(y, theta) computes this pmf in R\n\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\ncontinuous random variable: a random variable that takes uncountably many values.\nThe probability density function (pdf) of a continuous random variable, X is defined\n\\(pdf(x) = \\lim_{\\Delta x \\rightarrow 0} \\frac{p(x < X < x + \\Delta x)}{\\Delta x}\\)\nand the probability X is in some interval,\n\\(p(x_1 < X < x_2) = \\int_{x_1}^{x_2} pdf(x) dx\\)\nExamples\n\nNormal pdf \\[\np(x | \\mu, \\sigma) = (2\\pi \\sigma^2)^{-\\frac{1}{2}}e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}\n\\]\nUniform pdf \\[p(x|a,b) =\n\\begin{cases}\n\\frac{1}{b - a} \\hspace{.6cm}\\text{ for } x \\in [a, b]\\\\\n0 \\hspace{1cm}\\text{ otherwise }\n\\end{cases}\\]\n\n\n\nNote: we will often abuse notation and use \\(p(x)\\) in place of \\(pmf(x)\\) and \\(pdf(x)\\) and prob(event), where only the context makes meaning clear.\nFor pmfs\n\\[\n\\begin{aligned}\n0 \\leq p(y) \\leq 1\\\\\n\\sum_{y \\in \\mathcal{Y}} p(y) = 1\n\\end{aligned}\n\\]\nSimilarly, for pdfs,\n\\[\n\\begin{aligned}\n0 \\leq p(y) \\ \\text{and} \\\\\n\\int_{y \\in \\mathcal{Y}} p(y) = 1\n\\end{aligned}\n\\]\nNote: For a continuous random variable Y, p(y) can be larger than 1 and p(y) is not p(Y = y), which equals 0.\n\n\n\n\n\n\nDefinition\n\n\n\nThe part of the density/mass function that depends on the variable is called the kernel.\nExample\n\nthe kernel of the normal pdf is \\(e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat‚Äôs the kernel of a gamma random variable X?\nRecall: the pdf of a gamma distribution:\n\\[\np(x | \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha - 1} e^{-\\beta x}\n\\]"
  },
  {
    "objectID": "notes/probability.html#moments",
    "href": "notes/probability.html#moments",
    "title": "Probability",
    "section": "Moments",
    "text": "Moments\nFor a random variable X, the \\(n\\)th moment is defined as E(\\(X^n\\)).\nRecall, the expected value is defined for discrete random variable X,\n\\[\nE(X) = \\sum_{x \\in \\mathcal{X}} x p(x)\n\\]\nand for continuous random variable Y,\n\\[\nE(Y) = \\int_{-\\infty}^{\\infty} y p(y) dy\n\\]\nThe variance of a random variable, is also known as the second central moment and is defined\n\\[\nE(X - E(X))^2\n\\] or equivalently,\n\\[\nE(X^2) - E(X)^2\n\\]\nMore generally, the covariance between two random variables X and Y is defined\n\\[\nE[(X - E[X])(Y - E[Y])]\n\\]"
  },
  {
    "objectID": "notes/probability.html#exchangeability",
    "href": "notes/probability.html#exchangeability",
    "title": "Probability",
    "section": "Exchangeability",
    "text": "Exchangeability\n\noffline notes"
  },
  {
    "objectID": "notes/reliability.html",
    "href": "notes/reliability.html",
    "title": "Posterior summaries and reliability",
    "section": "",
    "text": "Posterior mode: sometimes called ‚ÄúMAP‚Äù or ‚Äúmaximum a posteriori‚Äù estimate, this quantity is given by \\(\\hat{\\theta} = \\arg \\max_{\\theta} p(\\theta | y)\\).\n\nNotice this unwinds to be \\(\\hat{\\theta} = \\arg \\max_{\\theta} p(y | \\theta) p(\\theta)\\).\n\n\n\n\n\n\n\nExercise\n\n\n\n\nShow that, for the uniform prior, \\(\\hat{\\theta} = y / n\\)\nCompare to maximum likelihood estimate (MLE); see notes on likelihoods\n\n\n\nOne way to report the reliability of the posterior mode is to look at the width of the posterior near the mode, which we can sometimes approximate with a Gaussian distribution:\n\\[\np(\\theta | y) \\approx C e^{\\frac{1}{2} \\frac{d^2L}{d\\theta^2}|_{\\hat{\\theta}} (\\theta - \\hat{\\theta})^2}\n\\]\nwhere \\(C\\) is a normalization constant and \\(L\\) is the log-posterior, \\(\\log p(\\theta | y)\\).\nTaken together, the fitted Gaussian with a mean equal to the posterior mode is called the Laplace approximation.\n\nLet‚Äôs derive the Laplace approximation offline"
  },
  {
    "objectID": "notes/reliability.html#confidence-regions",
    "href": "notes/reliability.html#confidence-regions",
    "title": "Posterior summaries and reliability",
    "section": "Confidence regions",
    "text": "Confidence regions\n\n\n\n\n\n\nDefinition\n\n\n\nLet \\(\\Phi\\) be the support of \\(\\theta\\). An interval \\((l(y), u(y)) \\subset \\Phi\\) has 95% posterior coverage if\n\\[\np(l(y) < \\theta < u(y) | y ) = 0.95\n\\]\nInterpretation: after observing \\(Y = y\\), our probability that \\(\\theta \\in (l(y), u(y))\\) is 95%.\nSuch an interval is called 95% posterior confidence interval (CI). It may also sometimes be referred to as a 95% ‚Äúcredible interval‚Äù to distinguish it from a frequentist CI.\n\n\nContrast posterior coverage to frequentist coverage:\n\n\n\n\n\n\nDefinition\n\n\n\nA random interval \\((l(Y), u(Y)\\)) has 95% frequentist coverage for \\(\\theta\\) if before data are observed,\n\\[\np(l(Y) < \\theta < u(Y) | \\theta) = 0.95\n\\]\nInterpretation: if \\(Y \\sim P_\\theta\\) then the probability that \\((l(Y), u(Y)\\) will cover \\(\\theta\\) is 0.95.\n\n\nIn practice, for many applied problems\n\\[\np(l(y) < \\theta < u(y) | y ) \\approx p(l(Y) < \\theta < u(Y) | \\theta)\n\\]\nsee section 3.1.2. in the book."
  },
  {
    "objectID": "notes/reliability.html#high-posterior-density",
    "href": "notes/reliability.html#high-posterior-density",
    "title": "Posterior summaries and reliability",
    "section": "High posterior density",
    "text": "High posterior density\n\n\n\n\n\n\nDefinition\n\n\n\nA \\(100 \\times (1-\\alpha)\\)% high posterior density (HPD) region is a set \\(s(y) \\subset \\Theta\\) such that\n\n\\(p(\\theta \\in s(y) | Y = y) = 1 - \\alpha\\)\nIf \\(\\theta_a \\in s(y)\\) and \\(\\theta_b \\not\\in s(y)\\), then \\(p(\\theta_a | Y = y) > p(\\theta_b | Y = y)\\)\n\n\n\n\nNote: all points inside an HPD region have higher posterior density than points outside the region.\n\n\n\n\n\n\n\nExercise\n\n\n\nIs the HPD region always an interval?"
  },
  {
    "objectID": "notes/prediction1.html",
    "href": "notes/prediction1.html",
    "title": "Prediction & Intro to Monte Carlo",
    "section": "",
    "text": "Load packages:"
  },
  {
    "objectID": "notes/prediction1.html#prediction-example",
    "href": "notes/prediction1.html#prediction-example",
    "title": "Prediction & Intro to Monte Carlo",
    "section": "Prediction example",
    "text": "Prediction example\nGeneral social survey (1998)\nSetup:\n\nSuppose \\(X_i = 1\\) if the ith person is happy. \\(X_i = 0\\) otherwise.\nLet \\(Y = \\sum_{i = 1}^{n} X_i\\), where \\(n\\) is the number of people sampled.\n\\(Y_i | \\theta \\sim \\text{binomial}(\\theta)\\) for some fixed \\(n\\).\n\\(\\theta \\sim \\text{uniform}(0, 1)\\)\n\nScenario: We sample \\(n = 10\\) people. \\(y = 6\\) are happy. If we sample another \\(n = 10\\), what is the probability that \\(\\tilde{y}\\) are happy?\nWe fundamentally want the posterior predictive distribution, \\(p(\\tilde{y} | y)\\).\nFollowing the offline notes, and given conditional independence, we want\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\int p(\\tilde{y} | \\theta) p(\\theta | y) d\\theta\n&=\n\\int {n \\choose \\tilde{y}}\n(\\theta)^\\tilde{y} (1-\\theta)^{n-\\tilde{y}}\n\\cdot\n\\frac{1}{\\text{B(y + 1, n - y + 1)}}\\theta^{y}(1-\\theta)^{n-y}\nd\\theta\\\\\n&= {n \\choose \\tilde{y}} \\frac{1}{\\text{B(y + 1, n - y + 1)}} \\int \\theta^{\\tilde{y}+y} \\cdot\n(1-\\theta)^{(n-\\tilde{y}) + (n - y)} d\\theta\\\\\n&= {n \\choose \\tilde{y}}\n\\frac{\\text{B}(\\tilde{y} + y + 1, 2n - y - \\tilde{y} + 1)}{\\text{B(y + 1, n - y + 1)}}\n\\end{aligned}\n\\]\nwhere \\(B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\). We can of course simplify, since this is really a bunch of factorials, but we can also naively use the beta() function in R and push forward.\n\nplotcode\n\n\n\n\n\n\n\n\n\n\ny = 6\nn = 10\n\n# posterior predictive probability of ytilde\nprobYT = function(ytilde) {\n  choose(n, ytilde) * \n    beta(ytilde + y + 1, (2*n) - y - ytilde + 1) / \n    beta(y + 1, n - y + 1)\n}\n\n# construct data frame\ndf = data.frame(ytilde = 0:10) %>%\n  mutate(postPredict = probYT(ytilde))\n\n# plot data frame\ndf %>%\n  ggplot(aes(x = ytilde, y = postPredict)) +\n  geom_bar(stat = 'identity') +\n  labs(x = TeX(\"$\\\\tilde{y}$\"), y = TeX(\"$p(\\\\tilde{y}|y)$\"),\n       title = \"Posterior predictive probability\") +\n  theme_bw()"
  },
  {
    "objectID": "notes/prediction1.html#monte-carlo-motivation",
    "href": "notes/prediction1.html#monte-carlo-motivation",
    "title": "Prediction & Intro to Monte Carlo",
    "section": "Monte Carlo motivation",
    "text": "Monte Carlo motivation\nGeneral social survey from the 90s gathered data on the number of children to women of two categories: those with and without a bachelor‚Äôs degree.\nSetup:\n\n\\(Y_{i1}\\): number of children of \\(i\\)th woman in group 1 (no bachelor‚Äôs)\n\\(Y_{i2}\\): number of children of \\(i\\)th woman in group 2 (bachelor‚Äôs)\n\nModel:\n\n\\(Y_{11}, \\ldots, Y_{n_1 1} | \\theta_1 \\overset{\\mathrm{iid}}{\\sim} \\text{Poisson}(\\theta_1)\\)\n\\(Y_{12} \\ldots, Y_{n_2 2} | \\theta_2 \\overset{\\mathrm{iid}}{\\sim} \\text{Poisson}(\\theta_2)\\)\n\nPrior:\n\n\\(\\theta_1 \\sim \\text{gamma}(2, 1)\\)\n\\(\\theta_2 \\sim \\text{gamma}(2, 1)\\)\n\nData:\n\n\\(n_1 = 111\\), \\(\\bar{y_1} = 1.95\\), \\(\\sum y_{i 1} = 217\\)\n\\(n_2 = 44\\), \\(\\bar{y_1} = 1.5\\), \\(\\sum y_{i 1} = 66\\)\n\nPosterior:\n\n\\(\\theta_1 | \\vec{y_1} \\sim \\text{gamma}(219, 112)\\)\n\\(\\theta_2 | \\vec{y_2} \\sim \\text{gamma}(68, 45)\\)\n\nWe already know how to compute\n\nposterior mean: \\(E~\\theta | y = \\alpha / \\beta\\) (shape, rate parameterization)\nposterior density (dgamma)\nposterior quantiles and confidence intervals (qgamma)\n\nWhat about‚Ä¶\n\n\\(p(\\theta \\in \\mathcal{A} | y)\\),\n\\(E~g(\\theta) | y\\),\n\\(Var~g(\\theta) | y\\)?\n\nWhat about posterior distribution of \\(|\\theta_1 - \\theta_2\\), \\(\\theta_1 / \\theta_2\\), \\(\\text{max} \\{\\theta_1, \\theta_2 \\}\\)?"
  },
  {
    "objectID": "notes/prediction1.html#monte-carlo-integration",
    "href": "notes/prediction1.html#monte-carlo-integration",
    "title": "Prediction & Intro to Monte Carlo",
    "section": "Monte Carlo integration",
    "text": "Monte Carlo integration\n\napproximates an integral by a stochastic average\nshines when other methods of integration are impossible (e.g.¬†high dimensional integration)\nworks because of law of large numbers: for a random variable \\(X\\), the sample mean \\(\\bar{x}_N\\) converges to the true mean \\(\\mu\\) as the number of samples \\(N\\) tends to infinity.\n\nThe key idea is: we obtain independent samples from the posterior,\n\\[\n\\theta^{(1)}, \\ldots \\theta^{(N)} \\overset{\\mathrm{iid}}{\\sim} p(\\theta |\\vec{y})\n\\]\nthen the empirical distribution of the samples approximates the posterior (approximation improves as \\(N\\) increases).\nRecall\n\\[\nE~g(\\theta)|y = \\int_\\mathcal{\\theta} g(\\theta) f_\\theta(\\theta | y)dx \\approx \\frac{1}{N} \\sum_{i = 1}^N g(\\theta^{(i)})\n\\]\nwhere \\(f_x(x)\\) is the probability density function for a random variable \\(X\\).\nThe law of large numbers says that if our samples \\(\\theta^{(i)}\\) are independent, \\(\\frac{1}{N} \\sum_{i = 1}^N g(\\theta^{(i)})\\) to \\(E~\\theta|y\\).\n\n\n\n\n\n\nNote\n\n\n\nIntegrals are expectations, and expectations are integrals."
  },
  {
    "objectID": "notes/prediction1.html#examples",
    "href": "notes/prediction1.html#examples",
    "title": "Prediction & Intro to Monte Carlo",
    "section": "Examples",
    "text": "Examples\n\n\\(\\theta_1 | \\vec{y_1} \\sim \\text{gamma}(219, 112)\\)\n\\(\\theta_2 | \\vec{y_2} \\sim \\text{gamma}(68, 45)\\)\n\n\n(1) proof of concept: the mean\n\nset.seed(123)\nN = 5000\nrgamma(N, shape = 219, rate = 112) %>%\n  mean()\n\n[1] 1.95294\n\n\nPretty close to the true mean, 1.9553571.\n\n\n(2) posterior of \\(\\theta_1 - \\theta_2\\)\n\nset.seed(123)\ntheta1 = rgamma(N, shape = 219, rate = 112)\ntheta2 = rgamma(N, shape = 68, rate = 45)\n\ndf = data.frame(diff = theta1 - theta2)\n\ndf %>%\n  ggplot(aes(x = diff)) + \n  geom_density() +\n  theme_bw() +\n  labs(x = TeX(\"$\\\\theta_1 - \\\\theta_2$\"),\n       y = TeX(\"$p(\\\\theta_1 - \\\\theta_2 | {y}_1, {y}_2)$\"))\n\n\n\n\n\n\n(3) \\(p(\\theta_1 - \\theta_2> .5)\\)\n\nmean(df$diff > .5)\n\n[1] 0.4106\n\n\n\nExerciseFull solutionQuick Monte Carlo\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\\(\\theta \\sim \\text{uniform}(0, 1)\\)\nLet \\(\\gamma = \\log \\theta\\)\nVisualize \\(p(\\gamma)\\) using Monte Carlo simulation, then show using the change of variables formula and plotting the closed form of the density.\n\n\n\n\n\n\n# sample from p(theta)\ntheta = runif(10000)\n\n# define transform function\nf = function(x) {\n  return(exp(x))\n}\n\n# create a df for each plot\ndf = data.frame(gamma = -7:0)\ndf2 = data.frame(gammaSamples = log(theta))\n\n# make plots\ndf %>%\n  ggplot(aes(x = gamma)) +\n  stat_function(fun = f, col = 'red', alpha = 0.5) +\n  geom_histogram(data = df2, aes(x = gammaSamples,\n                                 y = ..density..),\n               fill = 'steelblue', alpha = 0.5)\n\n\n\n\n\n\n\n# Just making the Monte Carlo part of the plot \n# in 3 lines\ntheta = runif(10000)\ngamma = log(theta)\nhist(gamma)"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "STA 360: Bayesian methods and modern statistics",
    "section": "",
    "text": "This course introduces Bayesian modeling and inference, motivated by real world examples. Course topics include Bayes‚Äô theorem, exchangeability, conjugate priors, Markov chain Monte Carlo (MCMC) approximation, Gibbs sampling, hierarchical modeling, Bayesian regression and generalized linear models. We compare and contrast Bayesian methods to the frequentist paradigm. By the end of this course students should feel comfortable (1) writing Bayesian models and, when appropriate, (2) sampling from the posterior using MCMC to make inference.\n\n\n\n\n\n\n\n\n\nContact\nOffice hours\nLocation\n\n\n\n\nDr.¬†Alexander Fisher\naaf29@duke.edu\nTu: 11:30am-1:30pm\nOld Chem 207\n\n\nCarol Wang\nzhuoqun.wang@duke.edu\nMo: 6:00pm-8:00pm\nZoom\n\n\nManny Mokel\nemmanuel.mokel@duke.edu\nTh: 3:00pm-5:00pm\nOld Chem 203B\n\n\nCaitrin Murphy\ncaitrin.murphy@duke.edu\nWe: 4:30pm-6:30pm\nOld Chem 025\n\n\n\n\n\n\n\n\n\nLecture\nTu/Th 10:05 - 11:20am\nOld Chemistry 116\n\n\nLab 01\nM 3:05pm - 4:20pm\nPerkins LINK 087 (Classroom 3)\n\n\nLab 02\nM 4:40pm - 5:55pm\nSocial Sciences 124\n\n\n\nCourse website: sta360-fa23.github.io\n\n\n\n\n\n\n\n\n\n\n\nA First Course in Bayesian Statistical Methods. As a Duke student, an electronic version of the book is freely available to you on Springer link. Check the errata at the link above.\nChapter summaries. I compile major take-away points from each section. Review these to help prepare for exams.\nWe will use the statistical software package R on homework asignments in this course. R is freely available at http://www.r-project.org/. RStudio, the popular IDE for R, is freely available at https://posit.co/downloads/.\n\n\n\n\nPart I: The Bayesian modeling toolkit\n\nReview of probability\nConjugate statistical models\nSemi-conjugate models and Gibbs sampling\n\nPart II: Statistical model building and analysis\n\nMultilevel models\nLinear regression\nGeneralized linear models\nDensity estimation and classification\n\n\n\n\n\n\n\n\n\n\n\nAssignment\nDescription\n\n\n\n\nHomework (40%)\nIndividual take-home assignments, submitted to Gradescope.\n\n\nMidterms (30%)\nTwo in-class exams.\n\n\nFinal exam (25%)\nCumulative final during final‚Äôs week.\n\n\nQuizzes (5%)\nIn-class pop quizzes.\n\n\n\nA \\(>= 93\\), A- \\(< 93\\), B+ \\(< 90\\), B \\(< 87\\), B- \\(< 83\\), C+ \\(<80\\), C \\(< 77\\), C- \\(< 73\\), D+ \\(< 70\\), D \\(< 67\\), D- \\(< 63\\), F \\(< 60\\)\n\n\n\n\n\n\nA note on quizzes\n\n\n\nOn random class days, there will be a brief quiz on the previous lectures. If you score \\(>60\\%\\) cumulatively on your final quiz grade, you will receive full participation credit. Your lowest two quizzes will also be dropped.\n\n\n\n\n\n\n\n\nA note on exams\n\n\n\nIf you miss either midterm 1 or midterm 2, and have an excused absence, your missing midterm grade will be replaced by your final exam grade. You must take at least 1 midterm and the final exam to pass the course.\n\n\n\n\n\n\n\n\n\nAcademic integrity\nBy enrolling in this course, you commit to upholding Duke‚Äôs community standard reproduced as follows:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\nAny violations of academic integrity will automatically result in a 0 for the assignment and will be reported to the Office of Student Conduct for further action. For the Exams and Quizzes, students are required to work alone. For the Homework assignments, students may work with a study group but each student must write up and submit their own answers.\nLate work\nLate homework may be submitted with 48 hours of the assignment deadline. Late homework submitted within 24 hours (even 1 minute late) will receive a 5% late penalty. Late work submitted between 24 to 48 hours of the deadline will receive a 10% late penalty. Work submitted after 48 hours will not be accepted. Exams cannot be turned in late and can only be excused under exceptional circumstances. The Duke policy for illness requires a short-term illness report or a letter from the Dean; except in emergencies, all other absenteeism must be approved in advance (e.g., an athlete who must miss class may be excused by prior arrangement for specific days). For emergencies, email notification is needed at the first reasonable time.\nErrors in grading\nErrors in grading must be brought to the attention of the TA or instructor during office hours within 1 week of receiving the grade."
  },
  {
    "objectID": "quizzes/quiz01.html",
    "href": "quizzes/quiz01.html",
    "title": "Quiz 1",
    "section": "",
    "text": "Exercise 1\nTRUE or FALSE: The beta prior is conjugate to a binomial likelihood.\n\n\nExercise 2\nTRUE or FALSE: \\(p(y|\\theta)\\) is the marginal likelihood.\n\n\nExercise 3\nIn Bayes‚Äô theorem (written below), which term is the ‚Äúnormalizing constant‚Äù?\n\\[\np(\\theta | y) = \\frac{p(y |\\theta) p(\\theta)}{\\int p(y, \\theta) d\\theta}\n\\]\n\n\nExercise 4\n\\[\n\\begin{aligned}\nX &\\sim gamma(k, \\theta)\\\\\np(x) &= \\frac{1}{\\Gamma(k) \\theta^k}x^{k-1}e^{-x/\\theta}\n\\end{aligned}\n\\]\nThe kernel of the distribution is ___.\n\n\n\n04:00"
  },
  {
    "objectID": "quizzes/quiz02.html",
    "href": "quizzes/quiz02.html",
    "title": "Quiz 2",
    "section": "",
    "text": "Exercise 1\nTRUE or FALSE: this is a 95% Bayesian confidence interval:\n\\[\np(l(y) < \\theta < u(y) | y) = 0.95\n\\]\n\n\nExercise 2\nWrite ‚Äúequals‚Äù or ‚Äúdoes not equal‚Äù in the blank below:\nIf\n\\[\nY | \\theta \\sim \\text{binomial}(n, \\theta),\n\\]\nthen \\(\\hat{\\theta}_{MLE}\\) ___ \\(\\hat{\\theta}_{MAP}\\) when \\(\\theta \\sim \\text{beta}(1, 1)\\).\n\n\nExercise 3\nTRUE or FALSE: high posterior density regions are always intervals.\n\n\n\n03:00"
  },
  {
    "objectID": "notes/MonteCarlo.html",
    "href": "notes/MonteCarlo.html",
    "title": "Monte Carlo Integration",
    "section": "",
    "text": "# load packages\nlibrary(tidyverse)\nlibrary(latex2exp)"
  },
  {
    "objectID": "slides/lab2.html#practice-exercise",
    "href": "slides/lab2.html#practice-exercise",
    "title": "Sensitivity to the prior and change of variables",
    "section": "Practice exercise",
    "text": "Practice exercise\nA cancer laboratory is estimating the rate of tumorigenesis in two strains of mice, \\(A\\) and \\(B\\). They have tumor count data for 10 mice in strain \\(A\\) and 13 mice in strain \\(B\\),\n\nyA = c(12, 9, 12, 14, 13, 13, 15, 8, 15, 6)\nyB = c(11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7)\n\nAssume\n\\[\n\\begin{aligned}\nY_A &\\sim \\text{Poisson}(\\theta_A)\\\\\nY_B &\\sim \\text{Poisson}(\\theta_B).\n\\end{aligned}\n\\]\n\nExercise 1Exercise 2\n\n\nLet\n\\[\n\\begin{aligned}\n\\theta_A &\\sim \\text{gamma}(120, 10)\\\\\n\\theta_B &\\sim \\text{gamma}(12, 1).\n\\end{aligned}\n\\]\n\nCompute \\(p(\\theta_B < \\theta_A ~|~ \\vec{y}_A, \\vec{y}_B)\\) via Monte Carlo sampling.\n\n\n\nLet\n\\[\n\\begin{aligned}\n\\theta_A &\\sim \\text{gamma}(120, 10)\\\\\n\\theta_B &\\sim \\text{gamma}(12\\cdot n_0, n_0).\n\\end{aligned}\n\\]\n\nFor a range of values of \\(n_0\\), obtain \\(p(\\theta_B < \\theta_A ~|~ \\vec{y}_A, \\vec{y}_B)\\).\nDescribe how sensitive conclusions about the event \\(\\{ \\theta_B < \\theta_A\\}\\) are to the prior distribution on \\(\\theta_B\\)."
  },
  {
    "objectID": "slides/lab2.html#practice-exercise-1",
    "href": "slides/lab2.html#practice-exercise-1",
    "title": "Sensitivity to the prior and change of variables",
    "section": "Practice exercise",
    "text": "Practice exercise\n\nLet \\(X \\sim \\text{Unif}(5, 10)\\)\nLet \\(Y = X^2\\)\n\nNotice that even though \\(X^2\\) is not a monotonic function everywhere, it is a monotonic function over the support of X.\nExercise: use the change of variables formula to derive \\(p(y)\\). Confirm with Monte Carlo simulation.\n\n\nShow solution\nlibrary(tidyverse)\n\nx = runif(100000, 5, 10)\ny = x^2\n\ndf = data.frame(y)\n\nf = function(y) {\n  return(.1/sqrt(y))\n}\n\ndf %>%\n  ggplot(aes(x = y)) + \n  stat_function(fun = f) +\n  geom_histogram(aes(x = y, y = ..density..),\n                 fill = 'steelblue', alpha = 0.5)\n\n\n\n\nüîó sta360-fa23.github.io"
  },
  {
    "objectID": "notes/MonteCarlo.html#monte-carlo-prediction",
    "href": "notes/MonteCarlo.html#monte-carlo-prediction",
    "title": "Monte Carlo Integration",
    "section": "Monte Carlo prediction",
    "text": "Monte Carlo prediction\n\nPrior predictive distribution\nWe can use Monte Carlo to sample new observation, \\(\\tilde{y}\\), from the prior predictive distribution\n\\[\np(\\tilde{y}) = \\int p(\\tilde{y}|\\theta)p(\\theta) d\\theta,\n\\]\nwhere we proceed by following the iterative procedure below\n1. sample theta_i from the prior p(theta)\n2. sample ytilde from p(ytilde | theta_i)\n3. repeat steps 1 and 2\n\nthis can be useful to see if a prior for \\(p(\\theta)\\) actually translate to reasonable prior beliefs about the data.\n\n\n\n\n\n\n\nExercise\n\n\n\nFor \\(p(\\theta) = \\text{gamma}(8,2)\\), plot \\(p(\\tilde{y})\\) assuming \\(\\tilde{y} | \\theta \\sim \\text{Poisson}(\\theta)\\).\n\n\n\n\n\n\n\nPosterior predictive distribution\nWe can also sample \\(\\tilde{y}\\) from the posterior predictive distribution,\n\\[\np(\\tilde{y} | y_1, \\ldots y_n) = \\int p(\\tilde{y}|\\theta) p(\\theta|y_1, \\ldots, y_n)d\\theta,\n\\]\nwhere the procedure is the same as before, except step 1 is replace with sampling \\(\\theta\\) from the posterior \\(p(\\theta | y_1,\\ldots, y_n)\\).\nThe resulting sequence \\((\\theta^{(1)}, \\tilde{y}^{(1)}), \\ldots, (\\theta^{(N)}, \\tilde{y}^{(N)})\\) constitutes \\(N\\) independent samples from the joint posterior of \\((\\theta, \\tilde{Y})\\). The sequence \\(\\tilde{y}^{(1)}, \\ldots, \\tilde{y}^{(N)})\\) constitutes \\(N\\) independent samples from the marginal posterior distribution of \\(\\tilde{Y}\\), aka the posterior predictive distribution."
  },
  {
    "objectID": "notes/MonteCarlo.html#posterior-predictive-model-checking",
    "href": "notes/MonteCarlo.html#posterior-predictive-model-checking",
    "title": "Monte Carlo Integration",
    "section": "Posterior predictive model checking",
    "text": "Posterior predictive model checking\nWe can assess the fit of a model by comparing the posterior predictive distribution to the empirical distribution.\n\nExample: is our Poisson model flawed?\n\n# load general social survey data\ngss = read_csv(\"https://sta360-fa23.github.io/data/gss.csv\")\n\n\ny1 = gss$CHILDS[gss$FEMALE == 1 &  gss$YEAR >= 1990  & gss$AGE == 40 & \n                   gss$DEGREE < 3 ]\ny1 = y1[!is.na(y1)]\nn = length(y1)\n\nWe are examining the number of children \\(Y_i\\) belonging to \\(n=\\) 111 40 year old women surveyed 1990 or later without a bachelor‚Äôs. These data come from the general social survey.\nSuppose\n\\[\n\\begin{aligned}\nY_i & \\sim \\text{Poisson}(\\theta)\\\\\n\\theta & \\sim \\text{gamma}(2, 1).\n\\end{aligned}\n\\]\nThe empirical and predictive distributions of the data are both plotted below.\n\nplotcode\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\n# posterior predictive distribution\nytotal = sum(y1)\na = 2 ; b = 1\nN = 10000\ntheta.post.mc = rgamma(N, ytotal + a, b + n)\ny1.mc = rpois(N, theta.post.mc)\n\n# data\ndf = data.frame(y1) # empirical\ndf2 = data.frame(y1.mc) # post predictive\n  \n# make plot\ndf %>%\n  ggplot(aes(x = y1)) +\n  geom_bar(aes(x = y1 + .15, y = (..count..)/sum(..count..),\n               fill = \"empirical\"), alpha = 0.6, width = 0.3) +\n  geom_bar(data = df2, \n                 aes(x = y1.mc -.15, y = (..count..) / sum(..count..),\n                     fill = \"predictive\"), alpha = 0.4, width = 0.3) +\n  labs(x = \"number of children\", \n       y = TeX(\"$p(Y_i = y_i)$\"),\n       fill = \"\") +\n  scale_x_continuous(breaks = c(0:7), labels = c(0:7),\n                     limits = c(-.5,7.5)) +\n  \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nLet \\(\\mathbf{y}\\) be a vector of length 111. Let \\(t(\\mathbf{y})\\) be the ratio of \\(2\\)s to \\(1\\)s in \\(\\mathbf{y}\\). For our observed data, this test statistic \\(t(\\mathbf{y}_{obs}) = 38 / 19 = 2\\). What is the tail probability \\(p(t(\\tilde{\\mathbf{Y}}) \\geq t(\\mathbf{y}_{obs}))\\) under the posterior predictive distribution?"
  },
  {
    "objectID": "quizzes/quiz03.html",
    "href": "quizzes/quiz03.html",
    "title": "Quiz 3",
    "section": "",
    "text": "Exercise 1\nWrite ‚Äúposterior‚Äù or ‚Äúprior‚Äù in the blank below:\n\\(\\int p(\\tilde{y}|\\theta) p(\\theta | y_1,\\ldots y_n)d\\theta\\) is a ___ predictive distribution.\n\n\nExercise 2\nExpand \\(p(\\theta | y)\\) using Bayes‚Äô rule (include the normalization constant).\n\n\nExercise 3\nTRUE or FALSE\nMonte Carlo integration error scales \\(\\mathcal{O}(\\frac{1}{\\sqrt{N}})\\) where \\(N\\) is the number of independent samples from the posterior.\n\n\n\n03:00"
  },
  {
    "objectID": "quizzes/quiz03.html#exercise-3",
    "href": "quizzes/quiz03.html#exercise-3",
    "title": "Quiz 3",
    "section": "Exercise 3",
    "text": "Exercise 3\nTRUE or FALSE\nMonte Carlo integration error scales \\(\\mathcal{O}(\\frac{1}{\\sqrt{N}})\\) where \\(N\\) is the number of independent samples from the posterior.\n\n\n\n03:00"
  },
  {
    "objectID": "notes/normalModel1.html",
    "href": "notes/normalModel1.html",
    "title": "The normal model",
    "section": "",
    "text": "# load packages\nlibrary(tidyverse)\nlibrary(latex2exp)"
  },
  {
    "objectID": "notes/MonteCarlo.html#monte-carlo-error",
    "href": "notes/MonteCarlo.html#monte-carlo-error",
    "title": "Monte Carlo Integration",
    "section": "Monte Carlo error",
    "text": "Monte Carlo error\n\nHow many values should we simulate?\nRecall: expected values are integrals, and integrals are expected values. Since central limit theorem (CLT) deals with expected values‚Ä¶\nRecall: CLT states that if \\(\\theta_i |\\vec{y}\\) iid with mean \\(\\theta\\) and finite variance \\(\\sigma^2\\), for \\(i \\in \\{1, \\ldots, N\\}\\), then the sample mean\n\\[\n\\bar{\\theta} \\sim N(\\theta, \\frac{\\sigma^2}{N} ).\n\\]\n\nHow to remember this/show this? Offline notes.\n\nSo to estimate \\(\\theta\\), we can generate \\(\\bar{\\theta}\\) by Monte Carlo simulation and report a confidence interval using quantiles of the normal given above in conjunction with the Monte Carlo standard error \\(\\frac{\\hat{\\sigma}}{\\sqrt{N}}\\)\nThis means we get convergence at the rate \\(\\mathcal{O}\\left(\\frac{1}{\\sqrt{N}}\\right)\\) regardless of the dimension of the integral!\nRecall:\n\nsd1 = pnorm(1) - pnorm(-1)\nsd2 = pnorm(2) - pnorm(-2)\nsd3 = pnorm(3) - pnorm(-3)\n\n\na 0.6826895% confidence interval can be obtained using \\(\\pm 1\\cdot \\hat{\\sigma}/\\sqrt{N}\\)\na 0.9544997% confidence interval can be obtained using \\(\\pm 2\\cdot \\hat{\\sigma}/\\sqrt{N}\\)\na 0.9973002% confidence interval can be obtained using \\(\\pm 3\\cdot \\hat{\\sigma}/\\sqrt{N}\\)\n\n\n\nExample\n\n# Let theta be \"x\" in the code below\nset.seed(123)\n\n# binomial(n, p)\nn = 20\np = 0.4\n\n# mean, variance, sd of a binomial(n, p)\nEX = n*p # 20*.4 = 8\nVarX = n*p*(1-p) # 20*.4*.6 = 4.8\nsdX = sqrt(VarX) # 2.19089\n\n# Monte Carlo sample of size N\nN = 100\nxSamples = rbinom(N, size = n, prob = p) \n\n# sample mean, var, sd\nxbar = mean(xSamples)\nxvar = var(xSamples)\nxsigma = sd(xSamples) # = sqrt(sum((xSamples - xbar)^2) / (N -1))\n\nse = xsigma / sqrt(N)\n\nlb = round(xbar - (2*se), 3)\nub = round(xbar + (2*se), 3)\n\nFor N = 100 Monte Carlo samples, The posterior mean of \\(\\theta\\) is \\(\\bar{\\theta} =\\) 8.01 with 95% confidence interval (7.57 8.45).\n\n\n\n\n\n\nExercise\n\n\n\nAbove we estimate \\(Var(\\theta)\\) to be 4.838 and the standard error for \\(N = 100\\) was 0.22.\nIf you wanted to state \\(p(\\theta \\in (\\hat{\\theta} \\pm 0.01)) = 0.95\\), how large would \\(N\\) have to be?\nCheck your answer by adjusting \\(N\\) above."
  },
  {
    "objectID": "notes/normalModel1.html#components-of-the-normal",
    "href": "notes/normalModel1.html#components-of-the-normal",
    "title": "The normal model",
    "section": "Components of the normal",
    "text": "Components of the normal\nLet \\(Y\\) be normally distributed with mean \\(\\theta\\) and variance \\(\\sigma^2\\). Mathematically,\n\\[\nY | \\theta, \\sigma^2 \\sim N(\\theta, \\sigma^2).\n\\]\nThe density\n\\[\n\\begin{aligned}\np(y ~|~ \\theta, \\sigma^2 ) &= (2\\pi\\sigma^2)^{-1/2} e^{-\\frac{1}{2\\sigma^2} (x-\\theta)^2},\\\\\ny &\\in \\mathbb{R},\\\\\n\\theta &\\in \\mathbb{R},\\\\\n\\sigma &\\in \\mathbb{R}^+.\n\\end{aligned}\n\\]\n\nVocabulary\n\nlocation, scale\n\n\\(\\theta\\) is called the ‚Äòlocation‚Äô parameter\n\\(\\sigma\\) is called the ‚Äòscale‚Äô parameter\n\n\n\nprecision\nNotice that every time \\(\\sigma^2\\) appears in the density, it is inverted. For this reason, the inverse variance has a special name, precision. Mathematically we will call precision \\(\\lambda^2 = \\frac{1}{\\sigma^2}\\). Intuitively, precision tells us how close \\(y_i\\) is to the mean \\(\\theta\\). (Large precision = small variance = closer).\n\n\n\nplots of normal densities\n\n\n\n\n\n\nWarning\n\n\n\nIn R, the arguments of pnorm, dnorm, rnorm are the mean and standard deviation (not the variance!)\n\n\n\nplotcode\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\nN = 10000\ny.mc = rnorm(N, mean = 3, sd = 2)\n\ndf = data.frame(y.mc)\n\ndf %>%\n  ggplot(aes(x = y.mc)) +\n  geom_histogram(aes(y = ..density..), alpha = 0.6, fill = 'steelblue') +\n  stat_function(fun = dnorm, args = list(mean = 3, sd = 2), aes(color = \"N(3, 4)\")) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), aes(color = \"N(0, 1)\")) +\n  theme_bw() +\n  labs(x = \"y\", y = \"density\", title = \"Normal densities\",\n       color = \"\")"
  },
  {
    "objectID": "notes/normalModel1.html#one-parmaeter-inference",
    "href": "notes/normalModel1.html#one-parmaeter-inference",
    "title": "The normal model",
    "section": "One parmaeter inference",
    "text": "One parmaeter inference"
  },
  {
    "objectID": "notes/normalModel1.html#normal-definition",
    "href": "notes/normalModel1.html#normal-definition",
    "title": "The normal model",
    "section": "Normal definition",
    "text": "Normal definition\nLet \\(Y\\) be normally distributed with mean \\(\\theta\\) and variance \\(\\sigma^2\\). Mathematically,\n\\[\nY | \\theta, \\sigma^2 \\sim N(\\theta, \\sigma^2).\n\\]\nThe density\n\\[\n\\begin{aligned}\np(y ~|~ \\theta, \\sigma^2 ) &= (2\\pi\\sigma^2)^{-1/2} e^{-\\frac{1}{2\\sigma^2} (x-\\theta)^2},\\\\\ny &\\in \\mathbb{R},\\\\\n\\theta &\\in \\mathbb{R},\\\\\n\\sigma &\\in \\mathbb{R}^+.\n\\end{aligned}\n\\]\n\nVocabulary\n\nlocation, scale\n\n\\(\\theta\\) is called the ‚Äòlocation‚Äô parameter\n\\(\\sigma\\) is called the ‚Äòscale‚Äô parameter\n\n\n\nprecision\nNotice that every time \\(\\sigma^2\\) appears in the density, it is inverted. For this reason, the inverse variance has a special name, precision. Mathematically we will call precision \\(\\lambda^2 = \\frac{1}{\\sigma^2}\\). Intuitively, precision tells us how close \\(y\\) is to the mean \\(\\theta\\). (Large precision = small variance = closer).\n\n\n\nplots of normal densities\n\n\n\n\n\n\nWarning\n\n\n\nIn R, the arguments of pnorm, dnorm, rnorm are the mean and standard deviation (not the variance!)\n\n\n\nplotcode\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\nN = 10000\ny.mc = rnorm(N, mean = 3, sd = 2)\n\ndf = data.frame(y.mc)\n\ndf %>%\n  ggplot(aes(x = y.mc)) +\n  geom_histogram(aes(y = ..density..), alpha = 0.6, fill = 'steelblue') +\n  stat_function(fun = dnorm, args = list(mean = 3, sd = 2), aes(color = \"N(3, 4)\")) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), aes(color = \"N(0, 1)\")) +\n  theme_bw() +\n  labs(x = \"y\", y = \"density\", title = \"Normal densities\",\n       color = \"\")"
  },
  {
    "objectID": "notes/normalModel1.html#background",
    "href": "notes/normalModel1.html#background",
    "title": "The normal model",
    "section": "Background",
    "text": "Background\n\nDefinition and vocabulary\nLet \\(Y\\) be normally distributed with mean \\(\\theta\\) and variance \\(\\sigma^2\\). Mathematically,\n\\[\nY | \\theta, \\sigma^2  \\sim N(\\theta, \\sigma^2).\n\\]\nThe density\n\\[\n\\begin{aligned}\np(y ~|~ \\theta, \\sigma^2 ) &= (2\\pi\\sigma^2)^{-1/2} e^{-\\frac{1}{2\\sigma^2} (x-\\theta)^2},\\\\\ny &\\in \\mathbb{R},\\\\\n\\theta &\\in \\mathbb{R},\\\\\n\\sigma &\\in \\mathbb{R}^+.\n\\end{aligned}\n\\]\n\nlocation, scale\n\n\\(\\theta\\) is called the ‚Äòlocation‚Äô parameter\n\\(\\sigma\\) is called the ‚Äòscale‚Äô parameter\n\n\n\nprecision\nNotice that every time \\(\\sigma^2\\) appears in the density, it is inverted. For this reason, the inverse variance \\((\\frac{1}{\\sigma^2})\\) has a special name, precision. Intuitively, precision tells us how close \\(y\\) is to the mean \\(\\theta\\). (Large precision = small variance = closer).\n\n\n\nplots of normal densities\n\n\n\n\n\n\nWarning\n\n\n\nIn R, the arguments of pnorm, dnorm, rnorm are the mean and standard deviation (not the variance!)\n\n\n\nplotcode\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\nN = 10000\ny.mc = rnorm(N, mean = 3, sd = 2)\n\ndf = data.frame(y.mc)\n\ndf %>%\n  ggplot(aes(x = y.mc)) +\n  geom_histogram(aes(y = ..density..), alpha = 0.6, fill = 'steelblue') +\n  stat_function(fun = dnorm, args = list(mean = 3, sd = 2), aes(color = \"N(3, 4)\")) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), aes(color = \"N(0, 1)\")) +\n  theme_bw() +\n  labs(x = \"y\", y = \"density\", title = \"Normal densities\",\n       color = \"\")"
  },
  {
    "objectID": "notes/normalModel1.html#bayesian-inference",
    "href": "notes/normalModel1.html#bayesian-inference",
    "title": "The normal model",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nIn general, we wish to make inference about \\(\\theta\\) and \\(\\sigma^2\\) after observing some data \\(y_1, \\ldots y_n\\) and thus are interested in the posterior \\(p(\\theta, \\sigma^2 | y_1, \\ldots y_n)\\). This is the standard task we have seen thus far, and requires us to specify a joint prior \\(p(\\theta, \\sigma^2)\\). Below, we will work to find a class of conjugate priors over \\(\\theta\\) and \\(\\sigma^2\\).\nWe can break up the joint posterior into two pieces from the axioms of probability:\n\\[\np(\\theta, \\sigma^2 | y_1, \\ldots y_n) = p(\\theta | \\sigma^2, y_1, \\ldots, y_n)p(\\sigma^2|y_1, \\ldots y_n)\n\\]\nThis suggests that we calculate the joint posterior by:\n\nfirst finding the full conditional of \\(\\theta\\): \\(p(\\theta| \\sigma^2, \\vec{y})\\)\nand then finding the marginal posterior of \\(\\sigma^2\\): \\(p(\\sigma^2 | \\vec{y})\\),\n\nwhere \\(\\vec{y} = \\{y_1, \\ldots y_n\\}\\).\n\nThe full conditional of \\(\\theta\\)\nBy Bayes‚Äô theorem,\n\\[\np(\\theta| \\sigma^2, \\vec{y}) \\propto \\underbrace{p(\\vec{y} |\\theta, \\sigma^2)}_{\\text{likelihood}} \\underbrace{p(\\theta|\\sigma^2)}_{\\text{prior}}.\n\\]\nTo arrive at the full conditional posterior of \\(\\theta\\), we must first specify a prior on \\(\\theta\\).\nConsidering we have a normal likelihood, what is a conjugate class of densities for \\(\\theta\\)?\n\n\n\n\n\n\nanswer\n\n\n\n\n\n\\(\\theta | \\sigma^2 \\sim N(\\mu_0, \\tau_0^2)\\) for some \\(\\mu_0 \\in \\mathbb{R}\\) and \\(\\tau_0^2 \\in \\mathbb{R}^+\\) is conjugate.\n\n\n\nWith the conjugate prior, our full conditional posterior \\(\\{ \\theta| \\sigma^2, \\vec{y} \\} \\sim N(\\mu_n, \\tau_n^2)\\) where\n\\[\n\\begin{aligned}\n\\mu_n &=\n\\frac{\\frac{1}{\\tau_0^2}\\mu_0 + \\frac{n}{\\sigma^2} \\bar{y}}{\n\\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}\n}\n\\\\\n\\\\\n\\tau_n^2 &= \\frac{1}{\\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}}\n\\end{aligned}\n\\]\n\nLet‚Äôs sketch out ‚Äòcompleting the square‚Äô to derive the parameters offline.\n\n\n\nIntuitive posterior parameters\nIf we consider the posterior precision, \\(\\frac{1}{\\tau_n^2}\\), we can re-arrange the terms above to illuminate how posterior information = prior information + data information;.\n\\[\n\\frac{1}{\\tau_n^2}= \\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}\n\\]\nIn words, posterior precision is equivalent to prior precision plus sampling precision. If we name each precision term, \\(\\lambda_0 = \\frac{1}{\\tau_0}\\), \\(\\lambda_n = \\frac{1}{\\tau_n}\\) and \\(\\lambda = \\frac{1}{\\sigma}\\) then\n\\[\n\\mu_n = \\frac{\\lambda_0^2}{\\lambda_0^2 + n\\lambda^2} \\mu_0 +\n\\frac{n\\lambda^2}{\\lambda_0^2 + n\\lambda^2} \\bar{y}\n\\]\ni.e.¬†the posterior mean is the weighted average of prior and sample mean, where the weights are the relative contribution of each precision!\nWe can re-define \\(\\lambda_0^2 = \\kappa_0 \\lambda^2\\) (or equivalently \\(\\tau_0^2 = \\frac{\\sigma^2}{\\kappa_0}\\)) and obtain\n\\[\n\\begin{aligned}\n\\mu_n &= \\frac{\\kappa_0}{\\kappa_0 + n} \\mu_0 + \\frac{n}{\\kappa_0 + n} \\bar{y},\\\\\n\\frac{1}{\\tau_n^2} &= \\frac{\\kappa_0 + n}{\\sigma^2}\n\\end{aligned}\n\\]\nwhere we can interpret \\(\\kappa_0\\) as the prior sample size.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that \\(\\tau_n^2\\) is the posterior variance of the full conditional posterior of \\(\\theta\\). This is distinct from \\(\\sigma_n^2\\), defined below.\n\n\n\n\nPrior on \\(\\sigma^2\\)\nRemember, we want \\(p(\\theta, \\sigma^2 | \\vec{y}) = p(\\theta | \\sigma^2, y_1, \\ldots, y_n)p(\\sigma^2|y_1, \\ldots y_n)\\). We have the first component of the right hand side, what about the second component?\nNotice that\n\\[\np(\\sigma^2 | \\vec{y}) \\propto p(\\sigma^2)\\int p(\\vec{y} | \\theta, \\sigma^2) p(\\theta|\\sigma^2) d\\theta\n\\]\nBut how do we choose \\(p(\\sigma^2)\\) to be conjugate? We can proceed in multiple ways: one is noting that the integral is really a convolution of normals, (thereby a sum of normals) and is therefore a normal density.\nUpon inspection, we can see a suitable choice is \\(\\frac{1}{\\sigma^2} \\sim \\text{gamma}(a, b)\\).\n\n\nThe inverse-gamma\nA random variable \\(X \\in (0, \\infty)\\) has an inverse-gamma(a,b) distribution if \\(\\frac{1}{X}\\) has a gamma(a,b) distribution.\nIf X has an inverse-gamma distribution, the density of X is\n\\[\np(x | a, b) = \\frac{b^a}{\\Gamma(a)} x^{-a-1}e^{-b/x} \\ \\text{for } \\ x > 0\n\\]\nand\n\\[\n\\begin{aligned}\nEX &= \\frac{b}{(a-1)} \\text{ if } a \\geq 1; \\ \\infty \\text{ if } 0<a<1,\\\\\nVar(X) &= \\frac{b^2}{(a-1)^2(a-2)} \\ \\text{if } a \\geq 2; \\ \\infty \\text{ if } 0 < a < 2,\\\\\nMode(X) &= \\frac{b}{a +1}.\n\\end{aligned}\n\\]\n\n\nThe marginal posterior of \\(\\sigma^2\\)\nTaken all together, if we let our sampling model and prior distributions be such that\n\\[\n\\begin{aligned}\nY_i | \\theta, \\sigma^2 &\\sim N(\\theta, \\sigma^2)\\\\\n\\theta | \\sigma^2 & \\sim N(\\mu_0, \\sigma^2/\\kappa_0)\\\\\n\\frac{1}{\\sigma^2} &\\sim \\text{gamma}(\\frac{\\nu_0}{2}, \\frac{\\nu_0}{2} \\sigma_0^2)\n\\end{aligned}\n\\]\nthen the posterior\n\\[\n\\frac{1}{\\sigma^2} | \\vec{y} \\sim \\text{gamma}(\\frac{\\nu_n}{2}, \\frac{\\nu_n \\sigma^2_n}{2}),\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\nu_n &= \\nu_0 + n,\\\\\n\\sigma^2_n &= \\frac{1}{\\nu_n} \\left[\n\\nu_0 \\sigma^2_0 +(n-1)s^2 + \\frac{\\kappa_0 n}{\\kappa_0 + n}(\\bar{y} - \\mu_0)^2\n\\right],\n\\end{aligned}\n\\]\nand \\(s^2\\) is the sample variance, \\(\\frac{1}{n-1} \\sum_i (y_i - \\bar{y})^2\\)."
  },
  {
    "objectID": "notes/normalModel1.html#the-marginal-posterior-of-sigma2",
    "href": "notes/normalModel1.html#the-marginal-posterior-of-sigma2",
    "title": "The normal model",
    "section": "The marginal posterior of \\(\\sigma^2\\)",
    "text": "The marginal posterior of \\(\\sigma^2\\)"
  },
  {
    "objectID": "notes/normalModel1.html#posterior-estimates-via-monte-carlo-sampling",
    "href": "notes/normalModel1.html#posterior-estimates-via-monte-carlo-sampling",
    "title": "The normal model",
    "section": "Posterior estimates via Monte Carlo sampling",
    "text": "Posterior estimates via Monte Carlo sampling\n\n# setting hyperparameters\nnu0 = 3; s20 = 1\nk0 = 1; mu0 = 2\n\n# data\ny = c(1.64, 1.70, 1.72, 1.82, 1.82, 1.82, 1.90, 2.08)\nn = length(y)\nybar = mean(y)\ns2 = var(y)\n\n# posterior via Monte Carlo sampling\n\nN = 10000\ns2.mc = 1 / rgamma(N, nu0 + n, )"
  },
  {
    "objectID": "notes/normalModel1.html#computing-the-joint-posterior",
    "href": "notes/normalModel1.html#computing-the-joint-posterior",
    "title": "The normal model",
    "section": "Computing the joint posterior",
    "text": "Computing the joint posterior\nSince \\(p(\\theta, \\sigma^2 | \\vec{y}) = p(\\theta | \\sigma^2, y_1, \\ldots, y_n)p(\\sigma^2|y_1, \\ldots y_n)\\), we can compute the joint posterior by sampling from \\(p(\\sigma^2|y_1, \\ldots y_n)\\) and then sampling from \\(p(\\theta | \\sigma^2, y_1, \\ldots, y_n)\\) to draw samples from the joint posterior.\n\nExample\nProof of concept\nWe have some data:\n\n# generating 10 samples from the population\ntrue.theta = 4\ntrue.sigma = 1\ny = rnorm(10, true.theta, true.sigma)\n\nybar = mean(y) # sample mean\nn = length(y) # sample size\ns2 = var(y) # sample variance\n\nWe make inference about \\(\\theta\\) and \\(\\sigma^2\\):\n\n# priors\n# theta prior\nmu_0 = 2; k_0 = 1\n# sigma2 prior\nnu_0 = 1; s2_0 = 0.010\n\n# posterior parameters\nkn = k_0 + n\nnun = nu_0 + n\nmun = (k_0 * mu_0 + n * ybar) /kn\ns2n = (nu_0 * s2_0 + (n - 1) * s2 + k_0 * n * (ybar - mu_0)^2 / (kn)) / (nun)\n\ns2.postsample = 1 / rgamma(10000, nun / 2, s2n * nun / 2)\ntheta.postsample = rnorm(10000, mun, sqrt(s2.postsample / kn))\n\ndf = data.frame(theta.postsample, s2.postsample)\n\ndf %>%\n  ggplot(aes(x = theta.postsample, y = s2.postsample)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") +\n  labs(x = TeX(\"$\\\\theta$\"),\n       y = TeX(\"$\\\\sigma^2$\"),\n       fill = TeX(\"$p(\\\\theta, \\\\sigma^2 | y_1, \\\\ldots y_n)$\")) +\n  theme_bw()"
  },
  {
    "objectID": "slides/lab3-review.html#exercise",
    "href": "slides/lab3-review.html#exercise",
    "title": "Exam practice",
    "section": "Exercise",
    "text": "Exercise\nPhysicists studying a radioactive substance measure the times at which the substance emits a particle. They will record \\(n+1\\) emissions and set \\(Y_1\\) to be the time elapsed between the first and second emission, \\(Y_2\\) to be the time elapsed between the second and third emission and so on. They will model the data as \\(Y_1, \\ldots Y_n | \\theta \\sim \\text{exponential}(\\theta)\\). The pdf of the exponential(\\(\\theta\\)) distribution is\n\\[\np(y |\\theta) = \\theta e^{-\\theta y} \\ \\text{ for } \\ y>0, \\ \\theta>0.\n\\]\nFor this distribution, \\(E[Y|\\theta] = \\frac{1}{\\theta}\\).\n(a). Write out the corresponding joint density \\(p(y_1, \\ldots, y_n | \\theta)\\) and simplify as much as possible. Justify each step of your calculation.\n(b). For fixed values of \\(y_1, \\ldots y_n\\), find the value of \\(\\theta\\) that maximizes \\(p(y_1, \\ldots, y_n | \\theta)\\) as a function of \\(\\theta\\), and call this maximizing value \\(\\hat{\\theta}\\). Hint: \\(\\hat{\\theta}\\) can also be found by maximizing \\(\\log p(y_1, \\ldots, y_n | \\theta)\\), which is easier to work with.\n(c). Suppose your information about \\(\\theta\\) can be described with a gamma(\\(a, b\\)) prior distribution for some values of \\(a\\) and \\(b\\). Write out the formula for \\(p(\\theta | y_1, \\ldots y_n)\\), up to a proportionality in \\(\\theta\\), and simplify as much as possible. From this, identify explicitly the posterior distribution of \\(\\theta\\) (i.e., write ‚Äúthe posterior is a blank distribution with parameter(s) blank)‚Äù.\n(d). Obtain the formula for \\(E[\\theta, y_1, \\ldots y_n]\\) as a function of \\(a, b n\\) and \\(y_1, \\ldots y_n\\), and try to write this as a function of the estimator \\(\\hat{\\theta}\\) you found in part (b). What does \\(E[\\theta | y_1,\\ldots,y_n]\\) get close to as \\(n\\) increases?\n\n\nüîó sta360-fa23.github.io"
  },
  {
    "objectID": "solutions/workshop.html",
    "href": "solutions/workshop.html",
    "title": "Workshopping",
    "section": "",
    "text": "Proof of concept\nWe have some data:\n\n# generating 10 samples from the population\ntrue.theta = 4\ntrue.sigma = 1\ny = rnorm(10, true.theta, true.sigma)\n\nybar = mean(y) # sample mean\nn = length(y) # sample size\ns2 = var(y) # sample variance\n\nWe make inference about \\(\\theta\\) and \\(\\sigma^2\\):\n\n# priors\n# theta prior\nmu_0 = 2; k_0 = 1\n# sigma2 prior\nnu_0 = 1; s2_0 = 0.010\n\n# posterior parameters\nkn = k_0 + n\nnun = nu_0 + n\nmun = (k_0 * mu_0 + n * ybar) /kn\ns2n = (nu_0 * s2_0 + (n - 1) * s2 + k_0 * n * (ybar - mu_0)^2 / (kn)) / (nun)\n\ns2.postsample = 1 / rgamma(10000, nun / 2, s2n * nun / 2)\ntheta.postsample = rnorm(10000, mun, sqrt(s2.postsample / kn))\n\ndf = data.frame(theta.postsample, s2.postsample)\n\ndf %>%\n  ggplot(aes(x = theta.postsample, y = s2.postsample)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") +\n  labs(x = TeX(\"$\\\\theta$\"),\n       y = TeX(\"$\\\\sigma^2$\"),\n       fill = TeX(\"$p(\\\\theta, \\\\sigma^2 | y_1, \\\\ldots y_n)$\")) +\n  theme_bw()"
  },
  {
    "objectID": "notes/exam-notes.html",
    "href": "notes/exam-notes.html",
    "title": "Exam notes",
    "section": "",
    "text": "A random variable \\(X \\in \\mathbb{R}\\) has a \\(N(\\theta, \\sigma^2)\\) distribution if \\(\\sigma^2 > 0\\) and\n\\(p(x | \\theta, \\sigma^2) = (2 \\pi \\sigma^2)^{-\\frac{1}{2}} e^{-\\frac{1}{2\\sigma^2}(x - \\theta)^2} \\ \\ \\ \\text{ for } -\\infty < x < \\infty.\\)\n\n\n\nA random variable \\(X \\in (0, \\infty)\\) has a gamma(a,b) distribution if \\(a > 0, b > 0\\) and\n\\(p(x |a,b) = \\frac{b^a}{\\Gamma(a)} x^{a - 1} e^{-bx} \\ \\ \\ \\text{ for } x > 0.\\)\n\\(E[X | a, b] = a/b\\), \\(Var[X | a,b] = a / b^2\\)\n\n\n\nA random variable \\(X \\in (0, \\infty)\\) has an inverse-gamma(a,b) distribution if 1/X has a gamma(a,b) distribution. If \\(X\\) is inverse-gamma(a,b) then the density of X is\n\\(p(x|a,b) = \\frac{b^a}{\\Gamma(a)} x^{-a-1} e^{-b/x} \\ \\ \\ \\text{ for } x > 0.\\)\n\\(E[X|a,b] = \\frac{b}{a-1}\\) if \\(a>=1\\), \\(\\infty\\) if \\(0<a<1\\)\n\\(Var[X|a,b] = \\frac{b^2}{(a-1)^2(a-2)}\\) if \\(a\\geq2\\), \\(\\infty\\) if \\(0<a<2\\)\n\n\n\nA random variable \\(X \\in \\{0, 1, \\ldots, n\\}\\) has a binomial\\((n, \\theta)\\) distribution if \\(\\theta \\in [0, 1]\\) and\n\\(p(X = x| \\theta, n) = {n \\choose x} \\theta^x (1- \\theta)^{n-x} \\ \\ \\ \\text{ for } x\\in \\{0, 1, \\ldots, n \\}\\)\n\\(E[X|\\theta] = n\\theta\\), \\(Var[X|\\theta] = n\\theta(1-\\theta)\\)\n\n\n\nA random variable \\(X \\in [0, 1]\\) has a beta(a,b) distribution if \\(a > 0, b > 0\\) and\n\\(p(x|a,b) = \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)} x^{a-1} (1-x)^{b-1} \\ \\ \\ \\text{ for } 0 \\leq x \\leq 1.\\)\n\\(E[X|a,b] = \\frac{a}{a + b}\\), \\(Var[X|a,b] = \\frac{ab}{(a + b + 1)(a + b)^2}\\)\n\n\n\nA random variable \\(X \\in \\{0, 1, 2, \\ldots \\}\\) has a Poisson(\\(\\theta\\)) distribution if \\(\\theta > 0\\) and\n\\(p(X = x | \\theta) = \\theta^x \\frac{e^{-\\theta}}{x!} \\ \\ \\ \\text{ for } x \\in \\{0, 1, 2, \\ldots\\}\\)\n\\(E[X|\\theta] = \\theta\\), \\(Var[X|\\theta] = \\theta\\)\n\n\n\nA random variable \\(X \\in [0, \\infty)\\) has a exponential(\\(\\theta\\)) distribution if \\(\\theta >0\\) and\n\\(p(x | \\theta) = \\theta e^{-\\theta x}\\)\n\\(E[X|\\theta] = \\frac{1}{\\theta}\\), \\(Var[X|\\theta] = \\frac{1}{\\theta^2}\\)"
  },
  {
    "objectID": "chapterSummaries.html#definitions-and-conjugacy",
    "href": "chapterSummaries.html#definitions-and-conjugacy",
    "title": "Chapter summaries",
    "section": "Definitions and conjugacy",
    "text": "Definitions and conjugacy\n\nBe able to define likelihood, prior, poserior, normalizing constant\n\n\n\n\n\n\n\nDefinition\n\n\n\nA prior \\(p(\\theta)\\) is said to be conjugate to the data generative model \\(p(y|\\theta)\\) if the family of the posterior is necessarily in the same family as the prior. In math, \\(p(\\theta)\\) is conjugate to \\(p(y|\\theta)\\) if\n\\[\np(\\theta) \\in \\mathcal{P} \\implies p(\\theta | y) \\in \\mathcal{P}\n\\]\n\n\n\nExamples of conjugate models: beta-binomial, gamma-Poisson."
  },
  {
    "objectID": "chapterSummaries.html#reliability",
    "href": "chapterSummaries.html#reliability",
    "title": "Chapter summaries",
    "section": "Reliability",
    "text": "Reliability\n\n\n\n\n\n\nDefinition\n\n\n\nLet \\(\\Phi\\) be the support of \\(\\theta\\). An interval \\((l(y), u(y)) \\subset \\Phi\\) has 95% posterior coverage if\n\\[\np(l(y) < \\theta < u(y) | y ) = 0.95\n\\]\nInterpretation: after observing \\(Y = y\\), our probability that \\(\\theta \\in (l(y), u(y))\\) is 95%.\nSuch an interval is called 95% posterior confidence interval (CI). It may also sometimes be referred to as a 95% ‚Äúcredible interval‚Äù to distinguish it from a frequentist CI.\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nA \\(100 \\times (1-\\alpha)\\)% high posterior density (HPD) region is a set \\(s(y) \\subset \\Theta\\) such that\n\n\\(p(\\theta \\in s(y) | Y = y) = 1 - \\alpha\\)\nIf \\(\\theta_a \\in s(y)\\) and \\(\\theta_b \\not\\in s(y)\\), then \\(p(\\theta_a | Y = y) > p(\\theta_b | Y = y)\\)\n\n\n\n\nExponential families\nIf density \\(p(y|\\theta)\\) can be written \\(h(y) c(\\phi) e^{\\phi t(y)}\\) for some transform \\(\\phi = f(\\theta)\\) we can say \\(p(y|\\theta)\\) belongs in the exponential family, and the conjugate prior is \\(p(\\phi | n_0, t_0) =c(\\phi)^{n_0} e^{n_0 t_0 \\phi}\\)."
  },
  {
    "objectID": "chapterSummaries.html#exponential-families",
    "href": "chapterSummaries.html#exponential-families",
    "title": "Chapter summaries",
    "section": "Exponential families",
    "text": "Exponential families\nIf density \\(p(y|\\theta)\\) can be written \\(h(y) c(\\phi) e^{\\phi t(y)}\\) for some transform \\(\\phi = f(\\theta)\\) we can say \\(p(y|\\theta)\\) belongs in the exponential family, and the conjugate prior is \\(p(\\phi) =c(\\phi)^{n_0} e^{n_0 t_0 \\phi}\\)."
  },
  {
    "objectID": "chapterSummaries.html#chapter-3",
    "href": "chapterSummaries.html#chapter-3",
    "title": "Chapter summaries",
    "section": "Chapter 3",
    "text": "Chapter 3\n\nDefinitions and conjugacy\n\nBe able to define likelihood, prior, posterior, normalizing constant\n\n\n\n\n\n\n\nDefinition\n\n\n\nA prior \\(p(\\theta)\\) is said to be conjugate to the data generative model \\(p(y|\\theta)\\) if the family of the posterior is necessarily in the same family as the prior. In math, \\(p(\\theta)\\) is conjugate to \\(p(y|\\theta)\\) if\n\\[\np(\\theta) \\in \\mathcal{P} \\implies p(\\theta | y) \\in \\mathcal{P}\n\\]\n\n\n\nExamples of conjugate models: beta-binomial, gamma-Poisson.\n\n\n\nReliability\n\n\n\n\n\n\nDefinition\n\n\n\nLet \\(\\Phi\\) be the support of \\(\\theta\\). An interval \\((l(y), u(y)) \\subset \\Phi\\) has 95% posterior coverage if\n\\[\np(l(y) < \\theta < u(y) | y ) = 0.95\n\\]\nInterpretation: after observing \\(Y = y\\), our probability that \\(\\theta \\in (l(y), u(y))\\) is 95%.\nSuch an interval is called 95% posterior confidence interval (CI). It may also sometimes be referred to as a 95% ‚Äúcredible interval‚Äù to distinguish it from a frequentist CI.\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nA \\(100 \\times (1-\\alpha)\\)% high posterior density (HPD) region is a set \\(s(y) \\subset \\Theta\\) such that\n\n\\(p(\\theta \\in s(y) | Y = y) = 1 - \\alpha\\)\nIf \\(\\theta_a \\in s(y)\\) and \\(\\theta_b \\not\\in s(y)\\), then \\(p(\\theta_a | Y = y) > p(\\theta_b | Y = y)\\)\n\n\n\n\n\nExponential families\nIf density \\(p(y|\\theta)\\) can be written \\(h(y) c(\\phi) e^{\\phi t(y)}\\) for some transform \\(\\phi = f(\\theta)\\) we can say \\(p(y|\\theta)\\) belongs in the exponential family, and the conjugate prior is \\(p(\\phi | n_0, t_0) =c(\\phi)^{n_0} e^{n_0 t_0 \\phi}\\). Note: the conjugate prior is given over \\(\\phi\\) and we‚Äôd have to transform back if we care about \\(p(\\theta)\\)."
  },
  {
    "objectID": "chapterSummaries.html#chapter-4",
    "href": "chapterSummaries.html#chapter-4",
    "title": "Chapter summaries",
    "section": "Chapter 4",
    "text": "Chapter 4\n\nPredictive distributions\nThe posterior predictive distribution,\n\\[\np(\\tilde{y} | y_1, \\ldots y_n) = \\int p(\\tilde{y}|\\theta) p(\\theta|y_1, \\ldots, y_n)d\\theta\n\\]\nwhen \\(Y | \\theta\\) conditionally iid.\nThe prior predictive distribution,\n\\[\np(\\tilde{y}) = \\int p(\\tilde{y}|\\theta) p(\\theta)d\\theta.\n\\]\nNotice both the posterior and prior predictive distributions are represented as integrals. Integrals are expectations. This means we can use Monte Carlo integration to approximate.\nTo approximate the posterior predictive distribution:\n\nsample from the posterior of theta, \\(p(\\theta|y_1,\\ldots y_n)\\)\nsample from data generative model \\(p(\\tilde{y}|\\theta)\\) for the values of theta sampled in (1).\n\nTo approximate the prior predictive distribution:\n\nsample from the prior of theta, \\(p(\\theta)\\)\nsample from the data generative model \\(p(\\tilde{y}|\\theta)\\) for the values of theta sampled in (1).\n\n\n\nMonte Carlo error\nSince Monte Carlo approximation can be viewed as a sample mean approximating an expected value, CLT applies.\nMore specifically, if \\(\\theta_i |\\vec{y}\\) iid with mean \\(\\theta\\) and finite variance \\(\\sigma^2\\), for \\(i \\in \\{1, \\ldots, N\\}\\), then the sample mean\n\\[\n\\bar{\\theta} \\sim N(\\theta, \\frac{\\sigma^2}{N} ).\n\\]\nand Monte Carlo estimates converge at a rate \\(\\mathcal{O}\\left(\\frac{1}{\\sqrt{N}}\\right)\\) regardless of the dimension of the integral!\n\n\nThe sampling view\nIf we have a posterior \\(p(\\theta | y_1, \\ldots y_n)\\) that we can sample from and we want some summary of the posterior‚Ä¶ e.g.¬†we want\n\n\\(p(\\theta < a)\\)\nquantiles of the posterior , or\nthe posterior of some transform \\(f(\\theta)\\),\n\nthen we can simply sample from the posterior to obtain an empirical approximation of the posterior and then report the empirical quantity of interest. This is also called Monte Carlo approximation.\nThe procedure can be written:\n\nsample from the posterior \\(p(\\theta |y_1, \\ldots y_n)\\) some large number of times and then\ncompute the quantity of interest"
  },
  {
    "objectID": "hw/hw04.html",
    "href": "hw/hw04.html",
    "title": "Homework 4",
    "section": "",
    "text": "Let\n\\[\n\\begin{aligned}\nY | \\theta, \\sigma^2 &\\sim N(\\theta, 1/\\gamma)\\\\\n\\theta | \\sigma^2 &\\sim N(\\mu_0, 1/\\gamma \\kappa_0)\\\\\n\\gamma &\\sim \\text{gamma}(a, b)\n\\end{aligned}\n\\]\nso \\(\\gamma\\) is the precision (inverse-variance) of the normal distribution.\n\nDerive and simplify the joint pdf \\(p(y_1, \\ldots y_n | \\theta, \\gamma)\\)\nDerive the posterior of the precision, \\(p(\\gamma| y_1, \\ldots y_n)\\).\nDerive the posterior of \\(\\theta\\), \\(p(\\theta | y_1, \\ldots y_n)\\)"
  },
  {
    "objectID": "hw/hw04.html#exercise-2",
    "href": "hw/hw04.html#exercise-2",
    "title": "Homework 4",
    "section": "Exercise 2",
    "text": "Exercise 2\nExercise 5.1 from Hoff. You can read in the data from the three schools with the R code below. Hint: the problem specification is the same as exercise 1, except \\(a = \\nu_0/2\\) and \\(b = \\nu_0 \\sigma_0^2/2\\).\n\nlibrary(tidyverse)\nschool1 = read_csv(\"https://sta360-fa23.github.io/data/school1.csv\")\nschool2 = read_csv(\"https://sta360-fa23.github.io/data/school2.csv\")\nschool3 = read_csv(\"https://sta360-fa23.github.io/data/school3.csv\")"
  },
  {
    "objectID": "notes/normalModel1.html#sampling-from-the-joint-posterior",
    "href": "notes/normalModel1.html#sampling-from-the-joint-posterior",
    "title": "The normal model",
    "section": "Sampling from the joint posterior",
    "text": "Sampling from the joint posterior\nSince \\(p(\\theta, \\sigma^2 | \\vec{y}) = p(\\theta | \\sigma^2, y_1, \\ldots, y_n)p(\\sigma^2|y_1, \\ldots y_n)\\), we can sample from the joint posterior by first sampling from \\(p(\\sigma^2|y_1, \\ldots y_n)\\) and then sampling from \\(p(\\theta | \\sigma^2, y_1, \\ldots, y_n)\\).\n\nExample\nProof of concept\nWe have some data:\n\n# generating 10 samples from the population\nset.seed(123)\ntrue.theta = 4\ntrue.sigma = 1\ny = rnorm(10, true.theta, true.sigma)\n\nybar = mean(y) # sample mean\nn = length(y) # sample size\ns2 = var(y) # sample variance\n\nWe make inference about \\(\\theta\\) and \\(\\sigma^2\\):\n\n# priors\n# theta prior\nmu_0 = 2; k_0 = 1\n# sigma2 prior\nnu_0 = 1; s2_0 = 0.010\n\n# posterior parameters\nkn = k_0 + n\nnun = nu_0 + n\nmun = (k_0 * mu_0 + n * ybar) /kn\ns2n = (nu_0 * s2_0 + (n - 1) * s2 + k_0 * n * (ybar - mu_0)^2 / (kn)) / (nun)\n\ns2.postsample = 1 / rgamma(10000, nun / 2, s2n * nun / 2)\ntheta.postsample = rnorm(10000, mun, sqrt(s2.postsample / kn))\n\ndf = data.frame(theta.postsample, s2.postsample)\n\ndf %>%\n  ggplot(aes(x = theta.postsample, y = s2.postsample)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") +\n  labs(x = TeX(\"$\\\\theta$\"),\n       y = TeX(\"$\\\\sigma^2$\"),\n       fill = TeX(\"$p(\\\\theta, \\\\sigma^2 | y_1, \\\\ldots y_n)$\")) +\n  theme_bw()"
  },
  {
    "objectID": "hw/hw04.html#exercise-3",
    "href": "hw/hw04.html#exercise-3",
    "title": "Homework 4",
    "section": "Exercise 3",
    "text": "Exercise 3\n3.12 from Hoff."
  }
]